{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5isf1AvLntq",
        "outputId": "11997efc-51fa-46fd-9f09-96020a8ce0b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting altair<6,>=4.0 (from streamlit)\n",
            "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
            "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (5.3.2)\n",
            "Collecting click<9,>=7.0 (from streamlit)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (24.2)\n",
            "Collecting pandas<3,>=1.4.0 (from streamlit)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (10.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.25.0)\n",
            "Collecting pyarrow>=7.0 (from streamlit)\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.31.0)\n",
            "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting toml<2,>=0.10.1 (from streamlit)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
            "  Downloading narwhals-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
            "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/9.7 MB 1.3 MB/s eta 0:00:08\n",
            "    --------------------------------------- 0.2/9.7 MB 2.3 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.5/9.7 MB 3.1 MB/s eta 0:00:03\n",
            "   - -------------------------------------- 0.5/9.7 MB 2.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/9.7 MB 2.8 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 1.0/9.7 MB 3.6 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.2/9.7 MB 3.5 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 1.4/9.7 MB 3.6 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 1.7/9.7 MB 4.1 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.9/9.7 MB 4.4 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.1/9.7 MB 4.2 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.3/9.7 MB 4.2 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 2.6/9.7 MB 4.3 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 2.9/9.7 MB 4.6 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.3/9.7 MB 4.7 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 3.5/9.7 MB 4.9 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 3.7/9.7 MB 4.8 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.1/9.7 MB 4.9 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 4.5/9.7 MB 5.1 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 4.8/9.7 MB 5.2 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 5.2/9.7 MB 5.4 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.5/9.7 MB 5.4 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.9/9.7 MB 5.5 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 6.3/9.7 MB 5.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 6.6/9.7 MB 5.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 6.7/9.7 MB 5.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 7.0/9.7 MB 5.7 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.4/9.7 MB 5.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 7.6/9.7 MB 5.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.0/9.7 MB 5.8 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 8.3/9.7 MB 5.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.6/9.7 MB 5.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 9.0/9.7 MB 5.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 9.4/9.7 MB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.7/9.7 MB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.7/9.7 MB 6.0 MB/s eta 0:00:00\n",
            "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
            "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl (25.3 MB)\n",
            "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/25.3 MB 6.5 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 0.7/25.3 MB 7.7 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 1.0/25.3 MB 7.0 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 1.2/25.3 MB 6.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 1.6/25.3 MB 7.3 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 2.0/25.3 MB 7.6 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 2.4/25.3 MB 7.8 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 2.9/25.3 MB 7.9 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 3.2/25.3 MB 7.7 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 3.4/25.3 MB 7.5 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 3.9/25.3 MB 7.7 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 4.1/25.3 MB 7.6 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 4.4/25.3 MB 7.3 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 4.7/25.3 MB 7.4 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 5.0/25.3 MB 7.2 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 5.2/25.3 MB 7.0 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 5.5/25.3 MB 7.0 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 5.9/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 6.2/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 6.5/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 6.7/25.3 MB 7.0 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 7.1/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 7.4/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 7.8/25.3 MB 7.2 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 8.2/25.3 MB 7.2 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 8.4/25.3 MB 7.1 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 8.5/25.3 MB 7.0 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 8.7/25.3 MB 6.8 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 8.8/25.3 MB 6.6 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 8.9/25.3 MB 6.4 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 9.0/25.3 MB 6.3 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 9.1/25.3 MB 6.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 9.2/25.3 MB 6.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 9.3/25.3 MB 6.0 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 9.5/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 9.7/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 10.0/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 10.2/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 10.5/25.3 MB 5.8 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 10.8/25.3 MB 5.8 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 11.2/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 11.5/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 11.9/25.3 MB 5.9 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 12.1/25.3 MB 5.8 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 12.5/25.3 MB 5.8 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 12.7/25.3 MB 5.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 13.0/25.3 MB 5.7 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 13.3/25.3 MB 5.6 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 13.5/25.3 MB 5.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 13.9/25.3 MB 5.7 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 14.2/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 14.5/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 14.7/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 15.0/25.3 MB 5.5 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 15.3/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 15.6/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 15.9/25.3 MB 5.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 16.0/25.3 MB 5.5 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 16.0/25.3 MB 5.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 16.5/25.3 MB 5.5 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 16.7/25.3 MB 5.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 16.8/25.3 MB 5.3 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 17.0/25.3 MB 5.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 17.2/25.3 MB 5.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 17.5/25.3 MB 5.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 17.7/25.3 MB 5.2 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 17.9/25.3 MB 5.1 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 18.1/25.3 MB 5.1 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 18.3/25.3 MB 5.1 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 18.5/25.3 MB 5.0 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 18.8/25.3 MB 5.1 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 19.0/25.3 MB 5.2 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 19.2/25.3 MB 5.3 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 19.3/25.3 MB 5.3 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 19.5/25.3 MB 5.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 19.7/25.3 MB 5.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 19.9/25.3 MB 5.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 20.0/25.3 MB 5.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 20.3/25.3 MB 5.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 20.5/25.3 MB 5.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 20.8/25.3 MB 5.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.0/25.3 MB 5.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.2/25.3 MB 5.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.4/25.3 MB 5.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 21.6/25.3 MB 5.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 21.8/25.3 MB 5.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.0/25.3 MB 5.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.1/25.3 MB 5.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.1/25.3 MB 4.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.2/25.3 MB 4.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.4/25.3 MB 4.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 22.6/25.3 MB 4.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 22.9/25.3 MB 4.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 23.1/25.3 MB 4.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 23.2/25.3 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.5/25.3 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.7/25.3 MB 4.7 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 24.0/25.3 MB 4.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 24.3/25.3 MB 4.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 24.5/25.3 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/25.3 MB 4.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.9/25.3 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.1/25.3 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.3/25.3 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.3/25.3 MB 4.4 MB/s eta 0:00:00\n",
            "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
            "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading narwhals-1.31.0-py3-none-any.whl (313 kB)\n",
            "   ---------------------------------------- 0.0/313.1 kB ? eta -:--:--\n",
            "   ------------------------------------ --- 286.7/313.1 kB 8.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 313.1/313.1 kB 6.4 MB/s eta 0:00:00\n",
            "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
            "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
            "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pytz, watchdog, tzdata, toml, tenacity, smmap, pyarrow, narwhals, click, blinker, pydeck, pandas, gitdb, gitpython, altair, streamlit\n",
            "Successfully installed altair-5.5.0 blinker-1.9.0 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 narwhals-1.31.0 pandas-2.2.3 pyarrow-19.0.1 pydeck-0.9.1 pytz-2025.1 smmap-5.0.2 streamlit-1.43.2 tenacity-9.0.0 toml-0.10.2 tzdata-2025.1 watchdog-6.0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai-whisper in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20240930)\n",
            "Requirement already satisfied: numba in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (2.5.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper) (0.8.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (3.1.5)\n",
            "Requirement already satisfied: fsspec in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
            "   -------- ------------------------------- 61.4/301.8 kB 3.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 204.8/301.8 kB 2.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 301.8/301.8 kB 2.3 MB/s eta 0:00:00\n",
            "Installing collected packages: joblib, nltk\n",
            "Successfully installed joblib-1.4.2 nltk-3.9.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
            "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/12.2 MB 1.1 MB/s eta 0:00:11\n",
            "    --------------------------------------- 0.2/12.2 MB 1.5 MB/s eta 0:00:09\n",
            "    --------------------------------------- 0.3/12.2 MB 1.6 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.3/12.2 MB 1.6 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.4/12.2 MB 1.5 MB/s eta 0:00:09\n",
            "   - -------------------------------------- 0.5/12.2 MB 1.6 MB/s eta 0:00:08\n",
            "   - -------------------------------------- 0.6/12.2 MB 1.7 MB/s eta 0:00:08\n",
            "   -- ------------------------------------- 0.7/12.2 MB 1.7 MB/s eta 0:00:07\n",
            "   -- ------------------------------------- 0.8/12.2 MB 1.7 MB/s eta 0:00:07\n",
            "   -- ------------------------------------- 0.9/12.2 MB 1.7 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 1.0/12.2 MB 1.8 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 1.1/12.2 MB 1.8 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 1.2/12.2 MB 1.9 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 1.3/12.2 MB 1.9 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 1.4/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 1.5/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 1.6/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 1.8/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 1.9/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 2.0/12.2 MB 2.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 2.1/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 2.2/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 2.3/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 2.4/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 2.6/12.2 MB 2.2 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 2.7/12.2 MB 2.2 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 2.7/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 2.8/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 2.9/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 2.9/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 3.1/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 3.2/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 3.2/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 3.4/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 3.5/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 3.6/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 3.7/12.2 MB 2.1 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 3.8/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 3.9/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 4.0/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 4.2/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.3/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.4/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.5/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.5/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   -------------- ------------------------- 4.5/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.6/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.7/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 4.8/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 4.9/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 5.0/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 5.2/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 5.3/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 5.4/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 5.5/12.2 MB 2.2 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 5.6/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 5.7/12.2 MB 2.1 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 5.8/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 6.0/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 6.1/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 6.2/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 6.3/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 6.5/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 6.6/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 6.7/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 6.9/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 7.0/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.1/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.2/12.2 MB 2.2 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.4/12.2 MB 2.3 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.4/12.2 MB 2.3 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.6/12.2 MB 2.3 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 7.7/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.9/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 8.0/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 8.1/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.3/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.3/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.5/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.6/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.8/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.9/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 9.1/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.2/12.2 MB 2.3 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.3/12.2 MB 2.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.5/12.2 MB 2.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.7/12.2 MB 2.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.8/12.2 MB 2.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 10.0/12.2 MB 2.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.2/12.2 MB 2.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.3/12.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 10.5/12.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 10.5/12.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.8/12.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.9/12.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.2/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.3/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.4/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.5/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.6/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.7/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.9/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.0/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.2/12.2 MB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.2/12.2 MB 2.6 MB/s eta 0:00:00\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
            "   ------------- -------------------------- 61.4/183.0 kB 1.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 183.0/183.0 kB 1.9 MB/s eta 0:00:00\n",
            "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
            "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
            "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
            "   --------------------------------- ------ 102.4/122.3 kB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 122.3/122.3 kB 2.4 MB/s eta 0:00:00\n",
            "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
            "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
            "   ------- -------------------------------- 112.6/632.6 kB 3.3 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 204.8/632.6 kB 2.5 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 368.6/632.6 kB 2.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 532.5/632.6 kB 3.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 632.6/632.6 kB 3.1 MB/s eta 0:00:00\n",
            "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.2/1.5 MB 3.5 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.3/1.5 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.5/1.5 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 0.6/1.5 MB 3.3 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 0.8/1.5 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.0/1.5 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.1/1.5 MB 3.2 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.2/1.5 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.3/1.5 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.5/1.5 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 3.2 MB/s eta 0:00:00\n",
            "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "   ---------------------------------------- 0.0/45.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 45.1/45.1 kB 2.2 MB/s eta 0:00:00\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
            "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.2/6.2 MB 3.9 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.4/6.2 MB 3.9 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 0.6/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 0.8/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.0/6.2 MB 4.1 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.1/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 1.3/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 1.5/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 1.8/6.2 MB 4.1 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 1.9/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 2.0/6.2 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 2.2/6.2 MB 4.0 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 2.4/6.2 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 2.6/6.2 MB 4.0 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 2.7/6.2 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 2.8/6.2 MB 3.8 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 2.9/6.2 MB 3.7 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 2.9/6.2 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 3.0/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 3.1/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 3.2/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 3.4/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 3.5/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.6/6.2 MB 3.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 3.7/6.2 MB 3.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 3.9/6.2 MB 3.2 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 4.1/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 4.3/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 4.5/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 4.5/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 4.8/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 5.0/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 5.1/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.2/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 5.4/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 5.6/6.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.7/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 5.9/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.1/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.2/6.2 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.2/6.2 MB 3.3 MB/s eta 0:00:00\n",
            "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "   ---------------------------------------- 0.0/52.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 52.7/52.7 kB ? eta 0:00:00\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.2/5.4 MB 3.1 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.3/5.4 MB 3.3 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 0.5/5.4 MB 3.3 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 0.7/5.4 MB 3.5 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 0.8/5.4 MB 3.8 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.0/5.4 MB 3.6 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 1.2/5.4 MB 3.9 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 1.4/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 1.6/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 1.8/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 2.0/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 2.2/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 4.1 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 2.6/5.4 MB 4.1 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 2.8/5.4 MB 4.1 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 2.9/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 3.1/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 3.3/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 3.4/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 3.7/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 3.8/5.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 3.9/5.4 MB 3.8 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 4.0/5.4 MB 3.7 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 4.2/5.4 MB 3.8 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 4.4/5.4 MB 3.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 4.6/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 4.8/5.4 MB 3.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 5.0/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 5.2/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  5.4/5.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.4/5.4 MB 3.9 MB/s eta 0:00:00\n",
            "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "   ---------------------------------------- 0.0/61.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 61.7/61.7 kB ? eta 0:00:00\n",
            "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
            "   ---------------------------------------- 0.0/152.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 152.0/152.0 kB 4.6 MB/s eta 0:00:00\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.48.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.2/11.1 MB 5.8 MB/s eta 0:00:02\n",
            "   - -------------------------------------- 0.5/11.1 MB 6.2 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.8/11.1 MB 6.1 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 1.1/11.1 MB 7.0 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 1.1/11.1 MB 7.0 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 1.3/11.1 MB 4.9 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 1.6/11.1 MB 5.2 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.9/11.1 MB 5.2 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.4/11.1 MB 5.8 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.8/11.1 MB 6.1 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.0/11.1 MB 5.9 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 3.5/11.1 MB 6.3 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.8/11.1 MB 6.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.1/11.1 MB 6.4 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 4.4/11.1 MB 6.5 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.5/11.1 MB 6.3 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 4.9/11.1 MB 6.3 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 5.2/11.1 MB 6.3 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 5.5/11.1 MB 6.4 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.0/11.1 MB 6.6 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 6.4/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 6.7/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 7.0/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.3/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.3/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.3/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.4/11.1 MB 6.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 8.3/11.1 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 8.8/11.1 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.1/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.1/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.3/11.1 MB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.9/11.1 MB 6.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.2/11.1 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.7/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.8/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.1/11.1 MB 6.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 6.5 MB/s eta 0:00:00\n",
            "Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
            "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.3/41.2 MB 8.9 MB/s eta 0:00:05\n",
            "    --------------------------------------- 0.6/41.2 MB 9.6 MB/s eta 0:00:05\n",
            "    --------------------------------------- 0.9/41.2 MB 7.1 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 1.3/41.2 MB 7.3 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 1.8/41.2 MB 8.0 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 2.0/41.2 MB 7.9 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 2.4/41.2 MB 7.5 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 2.8/41.2 MB 7.8 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 3.3/41.2 MB 8.1 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 3.5/41.2 MB 7.7 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 3.9/41.2 MB 8.1 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 4.5/41.2 MB 8.1 MB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 4.9/41.2 MB 8.5 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 5.5/41.2 MB 8.8 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 5.6/41.2 MB 8.8 MB/s eta 0:00:05\n",
            "   ----- ---------------------------------- 5.6/41.2 MB 8.8 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 6.5/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 7.0/41.2 MB 8.6 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 7.5/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 7.9/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 8.3/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 8.7/41.2 MB 8.6 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 9.0/41.2 MB 8.6 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 9.6/41.2 MB 8.8 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 10.0/41.2 MB 8.8 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 10.5/41.2 MB 8.8 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 10.8/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 11.0/41.2 MB 8.8 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 11.3/41.2 MB 8.6 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 11.7/41.2 MB 8.7 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 12.4/41.2 MB 9.1 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 12.8/41.2 MB 9.0 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 13.4/41.2 MB 9.1 MB/s eta 0:00:04\n",
            "   ------------- -------------------------- 13.8/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 14.3/41.2 MB 9.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 14.7/41.2 MB 9.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 15.2/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 15.5/41.2 MB 9.0 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 15.9/41.2 MB 9.8 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 16.0/41.2 MB 9.6 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 16.9/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 17.3/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 17.7/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 18.1/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 18.5/41.2 MB 9.2 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 18.9/41.2 MB 9.2 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 19.4/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 19.9/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 20.3/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 20.9/41.2 MB 9.4 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 21.4/41.2 MB 9.9 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 21.9/41.2 MB 10.1 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 22.4/41.2 MB 9.9 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 22.9/41.2 MB 9.9 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 23.4/41.2 MB 9.9 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 24.0/41.2 MB 9.9 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 24.5/41.2 MB 10.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 25.1/41.2 MB 10.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 25.6/41.2 MB 10.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 26.2/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 26.6/41.2 MB 10.9 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 27.0/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 27.3/41.2 MB 10.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 27.7/41.2 MB 10.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 28.2/41.2 MB 10.4 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 28.7/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 29.1/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 29.6/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 30.1/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 30.6/41.2 MB 10.6 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 31.2/41.2 MB 10.6 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 31.8/41.2 MB 10.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 32.3/41.2 MB 10.7 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 32.9/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 33.5/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 34.0/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 34.5/41.2 MB 10.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 34.9/41.2 MB 10.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 35.3/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 35.8/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 36.3/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 36.9/41.2 MB 10.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 37.5/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 38.0/41.2 MB 11.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 38.5/41.2 MB 11.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 38.9/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 39.3/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 39.8/41.2 MB 11.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 40.1/41.2 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 40.1/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  40.6/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  41.1/41.2 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  41.2/41.2 MB 10.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 41.2/41.2 MB 9.6 MB/s eta 0:00:00\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting librosa\n",
            "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa)\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.1.1)\n",
            "Collecting soundfile>=0.12.1 (from librosa)\n",
            "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
            "Collecting pooch>=1.1 (from librosa)\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting soxr>=0.3.2 (from librosa)\n",
            "  Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (4.12.2)\n",
            "Collecting lazy_loader>=0.1 (from librosa)\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting msgpack>=1.0 (from librosa)\n",
            "  Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.7.22)\n",
            "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/260.7 kB ? eta -:--:--\n",
            "   ---- ----------------------------------- 30.7/260.7 kB ? eta -:--:--\n",
            "   ------ -------------------------------- 41.0/260.7 kB 653.6 kB/s eta 0:00:01\n",
            "   ------ -------------------------------- 41.0/260.7 kB 653.6 kB/s eta 0:00:01\n",
            "   --------- ----------------------------- 61.4/260.7 kB 365.7 kB/s eta 0:00:01\n",
            "   ------------- ------------------------- 92.2/260.7 kB 375.8 kB/s eta 0:00:01\n",
            "   ----------------- -------------------- 122.9/260.7 kB 425.1 kB/s eta 0:00:01\n",
            "   ----------------------- -------------- 163.8/260.7 kB 517.2 kB/s eta 0:00:01\n",
            "   ----------------------------- -------- 204.8/260.7 kB 541.9 kB/s eta 0:00:01\n",
            "   -------------------------------------  256.0/260.7 kB 605.3 kB/s eta 0:00:01\n",
            "   -------------------------------------- 260.7/260.7 kB 593.6 kB/s eta 0:00:00\n",
            "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl (74 kB)\n",
            "   ---------------------------------------- 0.0/74.9 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 30.7/74.9 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 74.9/74.9 kB 1.0 MB/s eta 0:00:00\n",
            "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "   ---------------------------------------- 0.0/64.6 kB ? eta -:--:--\n",
            "   ------------------------- -------------- 41.0/64.6 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 64.6/64.6 kB 1.2 MB/s eta 0:00:00\n",
            "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.0/1.0 MB 1.9 MB/s eta 0:00:01\n",
            "   --- ------------------------------------ 0.1/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 0.1/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.2/1.0 MB 1.1 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.2/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.3/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 0.4/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 0.4/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 0.5/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 0.5/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 0.6/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 0.7/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 0.7/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 0.8/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 0.8/1.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 0.9/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 1.0/1.0 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.0/1.0 MB 1.3 MB/s eta 0:00:00\n",
            "Downloading soxr-0.5.0.post1-cp311-cp311-win_amd64.whl (166 kB)\n",
            "   ---------------------------------------- 0.0/166.7 kB ? eta -:--:--\n",
            "   ----------------- ---------------------- 71.7/166.7 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  163.8/166.7 kB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 166.7/166.7 kB 1.7 MB/s eta 0:00:00\n",
            "Installing collected packages: soxr, msgpack, lazy_loader, audioread, soundfile, pooch, librosa\n",
            "Successfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 msgpack-1.1.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.68.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.0.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.25.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.9.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=2.0.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.0.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.0)\n",
            "Collecting sounddevice>=0.5.1 (from openai)\n",
            "  Downloading sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (0.18.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: CFFI>=1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sounddevice>=0.5.1->openai) (1.17.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: pycparser in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.5.1->openai) (2.22)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Downloading openai-1.68.0-py3-none-any.whl (605 kB)\n",
            "   ---------------------------------------- 0.0/605.6 kB ? eta -:--:--\n",
            "   ------- -------------------------------- 112.6/605.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 266.2/605.6 kB 2.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 583.7/605.6 kB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 605.6/605.6 kB 4.2 MB/s eta 0:00:00\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.9.0-cp311-cp311-win_amd64.whl (210 kB)\n",
            "   ---------------------------------------- 0.0/210.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 210.1/210.1 kB 6.4 MB/s eta 0:00:00\n",
            "Downloading sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
            "   ---------------------------------------- 0.0/363.6 kB ? eta -:--:--\n",
            "   --------------------------------- ------ 307.2/363.6 kB 6.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 363.6/363.6 kB 4.5 MB/s eta 0:00:00\n",
            "Installing collected packages: jiter, distro, sounddevice, openai\n",
            "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.68.0 sounddevice-0.5.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (pyproject.toml): started\n",
            "  Building wheel for fpdf (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40769 sha256=92e0cac7ae9734e805fd0ad00e397a9fb5e4437e21f90a786a21181637712679\n",
            "  Stored in directory: c:\\users\\jay kanavia\\appdata\\local\\pip\\cache\\wheels\\65\\4f\\66\\bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from reportlab) (10.1.0)\n",
            "Collecting chardet (from reportlab)\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Downloading reportlab-4.3.1-py3-none-any.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.1/1.9 MB 2.6 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 0.3/1.9 MB 3.5 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.5/1.9 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.7/1.9 MB 3.8 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 1.0/1.9 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.2/1.9 MB 4.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.5/1.9 MB 4.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.9/1.9 MB 5.4 MB/s eta 0:00:00\n",
            "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
            "   --------------------------------------- 199.4/199.4 kB 11.8 MB/s eta 0:00:00\n",
            "Installing collected packages: chardet, reportlab\n",
            "Successfully installed chardet-5.2.0 reportlab-4.3.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting markdown\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "   ---------------------------------------- 0.0/106.3 kB ? eta -:--:--\n",
            "   ---------------------------------- ----- 92.2/106.3 kB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 106.3/106.3 kB 2.0 MB/s eta 0:00:00\n",
            "Installing collected packages: markdown\n",
            "Successfully installed markdown-3.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (pyproject.toml): started\n",
            "  Building wheel for fuzzy (pyproject.toml): finished with status 'error'\n",
            "Failed to build fuzzy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "   Building wheel for fuzzy (pyproject.toml) did not run successfully.\n",
            "   exit code: 1\n",
            "  > [28 lines of output]\n",
            "      WARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ow9z7t4z\\normal\\Lib\\site-packages\\setuptools_scm\\_integration\\pyproject_reading.py\", line 36, in read_pyproject\n",
            "          section = defn.get(\"tool\", {})[tool_name]\n",
            "                    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
            "      KeyError: 'setuptools_scm'\n",
            "      C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ow9z7t4z\\overlay\\Lib\\site-packages\\setuptools\\dist.py:760: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "      \n",
            "              License :: OSI Approved :: MIT License\n",
            "              License :: OSI Approved :: Artistic License\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        self._finalize_license_expression()\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      [1/1] Cythonizing src/fuzzy.pyx\n",
            "      C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ow9z7t4z\\normal\\Lib\\site-packages\\Cython\\Compiler\\Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-install-wysm86il\\fuzzy_efdfce878431462b838d7a85793de2b7\\src\\fuzzy.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      building 'fuzzy' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for fuzzy\n",
            "ERROR: Could not build wheels for fuzzy, which is required to install pyproject.toml-based projects\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.1.3-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
            "Downloading jellyfish-1.1.3-cp311-cp311-win_amd64.whl (212 kB)\n",
            "   ---------------------------------------- 0.0/212.1 kB ? eta -:--:--\n",
            "   ----------------- ---------------------- 92.2/212.1 kB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 212.1/212.1 kB 3.3 MB/s eta 0:00:00\n",
            "Installing collected packages: jellyfish\n",
            "Successfully installed jellyfish-1.1.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl (100 kB)\n",
            "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 81.9/100.4 kB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 100.4/100.4 kB 1.9 MB/s eta 0:00:00\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-win_amd64.whl (1.6 MB)\n",
            "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.2/1.6 MB 5.0 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 0.5/1.6 MB 5.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 0.8/1.6 MB 6.6 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.1/1.6 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.1/1.6 MB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.2/1.6 MB 4.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.3/1.6 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  1.6/1.6 MB 4.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.6/1.6 MB 4.3 MB/s eta 0:00:00\n",
            "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.12.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Jay\n",
            "[nltk_data]     Kanavia\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jay\n",
            "[nltk_data]     Kanavia\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Jay\n",
            "[nltk_data]     Kanavia\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB 1.3 MB/s eta 0:00:10\n",
            "     --------------------------------------- 0.0/12.8 MB 653.6 kB/s eta 0:00:20\n",
            "     ---------------------------------------- 0.1/12.8 MB 1.2 MB/s eta 0:00:11\n",
            "      --------------------------------------- 0.2/12.8 MB 1.4 MB/s eta 0:00:10\n",
            "      --------------------------------------- 0.3/12.8 MB 1.5 MB/s eta 0:00:09\n",
            "     - -------------------------------------- 0.4/12.8 MB 1.5 MB/s eta 0:00:09\n",
            "     - -------------------------------------- 0.5/12.8 MB 1.6 MB/s eta 0:00:08\n",
            "     - -------------------------------------- 0.6/12.8 MB 1.6 MB/s eta 0:00:08\n",
            "     -- ------------------------------------- 0.7/12.8 MB 1.7 MB/s eta 0:00:08\n",
            "     -- ------------------------------------- 0.8/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "     -- ------------------------------------- 0.9/12.8 MB 1.8 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 1.0/12.8 MB 1.8 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 1.1/12.8 MB 1.9 MB/s eta 0:00:07\n",
            "     --- ------------------------------------ 1.2/12.8 MB 1.9 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.4/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.5/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ----- ---------------------------------- 1.7/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ----- ---------------------------------- 1.9/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 1.9/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 2.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.9 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 2.2/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 2.3/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 2.4/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 2.5/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.7/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.8/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 2.9/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 3.0/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 3.2/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.3/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.5/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 3.6/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 3.9/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 4.0/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 4.1/12.8 MB 2.1 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 4.2/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 4.3/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 4.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 4.6/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 4.7/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 4.8/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 4.8/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 4.9/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 5.1/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 5.4/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 5.7/12.8 MB 2.3 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 5.8/12.8 MB 2.3 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 5.9/12.8 MB 2.3 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 6.0/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 6.2/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 6.3/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 6.5/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 6.6/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 6.7/12.8 MB 2.3 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 6.9/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.2/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 7.5/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 7.6/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 7.8/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 7.9/12.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 8.1/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 8.2/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     -------------------------- ------------- 8.4/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     -------------------------- ------------- 8.6/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 8.7/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 8.9/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     ---------------------------- ----------- 9.0/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     ---------------------------- ----------- 9.2/12.8 MB 2.5 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 2.6 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 9.5/12.8 MB 2.6 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 9.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 9.9/12.8 MB 2.6 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.0/12.8 MB 2.6 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.2/12.8 MB 2.6 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 10.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 10.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.7/12.8 MB 2.7 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.9/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.1/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.2/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.4/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.6/12.8 MB 2.8 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.8/12.8 MB 2.9 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.0/12.8 MB 2.9 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 3.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.3/12.8 MB 3.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.5/12.8 MB 3.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.7/12.8 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.8/12.8 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 3.1 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install streamlit\n",
        "!pip install openai-whisper\n",
        "!pip install torch\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install transformers\n",
        "!pip install scikit-learn\n",
        "!pip install librosa\n",
        "!pip install openai\n",
        "!pip install fpdf\n",
        "!pip install reportlab\n",
        "!pip install markdown\n",
        "!pip install beautifulsoup4\n",
        "!pip install fuzzy\n",
        "!pip install jellyfish\n",
        "!pip install python-Levenshtein\n",
        "\n",
        "# Download required NLTK and spaCy data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzy\n",
            "  Using cached Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (pyproject.toml): started\n",
            "  Building wheel for fuzzy (pyproject.toml): finished with status 'error'\n",
            "Failed to build fuzzy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "   Building wheel for fuzzy (pyproject.toml) did not run successfully.\n",
            "   exit code: 1\n",
            "  > [28 lines of output]\n",
            "      WARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ecbjidda\\normal\\Lib\\site-packages\\setuptools_scm\\_integration\\pyproject_reading.py\", line 36, in read_pyproject\n",
            "          section = defn.get(\"tool\", {})[tool_name]\n",
            "                    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
            "      KeyError: 'setuptools_scm'\n",
            "      C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ecbjidda\\overlay\\Lib\\site-packages\\setuptools\\dist.py:760: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "      \n",
            "              License :: OSI Approved :: MIT License\n",
            "              License :: OSI Approved :: Artistic License\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        self._finalize_license_expression()\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      [1/1] Cythonizing src/fuzzy.pyx\n",
            "      C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-build-env-ecbjidda\\normal\\Lib\\site-packages\\Cython\\Compiler\\Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: C:\\Users\\Jay Kanavia\\AppData\\Local\\Temp\\pip-install-enby2969\\fuzzy_6a9af8913deb403f998abb1da536be45\\src\\fuzzy.pyx\n",
            "        tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "      building 'fuzzy' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for fuzzy\n",
            "ERROR: Failed to build installable wheels for some pyproject.toml based projects (fuzzy)\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbEVQchCL8Sy",
        "outputId": "77a665d2-45cb-425f-ab2e-4250b5733d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import whisper\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import gc\n",
        "import openai\n",
        "import librosa\n",
        "from datetime import timedelta\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "import base64\n",
        "from fpdf import FPDF\n",
        "import textwrap\n",
        "import markdown\n",
        "from bs4 import BeautifulSoup\n",
        "import io\n",
        "import unicodedata\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable, ListItem\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.enums import TA_LEFT, TA_CENTER\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.lib import colors\n",
        "import fuzzy  # For Double Metaphone algorithm\n",
        "import jellyfish  # For additional phonetic algorithms and string similarity\n",
        "import Levenshtein  # For edit distance calculation\n",
        "from difflib import SequenceMatcher  # For string similarity\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('words', quiet=True)\n",
        "nltk.download('names', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://models.inference.ai.azure.com\"\n",
        "MODEL = \"gpt-4o\"\n",
        "GITHUB_TOKEN = \"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\"\n",
        "# Replace with your actual token\n",
        "\n",
        "class NameStandardizer:\n",
        "    \"\"\"Class for standardizing human names using phonetic algorithms with improved filtering\"\"\"\n",
        "    def __init__(self):\n",
        "        self.name_mapping = {}  # Maps variants to standardized names\n",
        "        self.name_groups = {}   # Groups similar names by phonetic code\n",
        "        self.metaphone_mapping = {}  # Maps metaphone codes to standard names\n",
        "        self.person_context = {} # Stores context about people (roles, etc.)\n",
        "\n",
        "        # Initialize spaCy\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Load common human names datasets\n",
        "        self.load_name_datasets()\n",
        "\n",
        "        # Create an extended set of English words (non-names)\n",
        "        self.english_words = set(nltk.corpus.words.words())\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "\n",
        "        # Common words that are often capitalized but not names\n",
        "        self.common_capitalized = {\n",
        "            \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n",
        "            \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\",\n",
        "            \"September\", \"October\", \"November\", \"December\", \"The\", \"A\", \"An\", \"And\", \"But\",\n",
        "            \"Or\", \"For\", \"Nor\", \"So\", \"Yet\", \"I\", \"Internet\", \"World\", \"Web\", \"Email\", \"Yes\",\n",
        "            \"No\", \"Ok\", \"Okay\", \"Hi\", \"Hello\", \"Thanks\", \"Thank\", \"You\", \"Please\", \"University\",\n",
        "            \"College\", \"School\", \"Hospital\", \"Bank\", \"Station\", \"Airport\", \"Government\",\n",
        "            \"Company\", \"Corporation\", \"Department\", \"Association\", \"Organization\"\n",
        "        }\n",
        "\n",
        "    def load_name_datasets(self):\n",
        "        \"\"\"Load common first and last name datasets including multicultural names\"\"\"\n",
        "        # Get male and female names from NLTK (primarily Western names)\n",
        "        male_names = set(nltk.corpus.names.words('male.txt'))\n",
        "        female_names = set(nltk.corpus.names.words('female.txt'))\n",
        "\n",
        "        # Add common Indian names\n",
        "        indian_names = {\n",
        "            \"Aarav\", \"Vivaan\", \"Aditya\", \"Vihaan\", \"Arjun\", \"Reyansh\", \"Ayaan\", \"Atharva\",\n",
        "            \"Krishna\", \"Ishaan\", \"Shaurya\", \"Advik\", \"Rudra\", \"Kabir\", \"Anik\", \"Armaan\",\n",
        "            \"Dhruv\", \"Yuvan\", \"Virat\", \"Rohan\", \"Veer\", \"Gaurav\", \"Aahan\", \"Arnav\",\n",
        "            \"Aanya\", \"Aadhya\", \"Aaradhya\", \"Ananya\", \"Pari\", \"Anika\", \"Navya\", \"Diya\",\n",
        "            \"Avani\", \"Anaya\", \"Sara\", \"Myra\", \"Saanvi\", \"Ira\", \"Disha\", \"Kiara\",\n",
        "            \"Riya\", \"Aahana\", \"Anvi\", \"Prisha\", \"Siya\", \"Divya\", \"Avni\", \"Mahika\",\n",
        "            \"Sharma\", \"Patel\", \"Singh\", \"Kumar\", \"Gupta\", \"Kaur\", \"Shah\", \"Mehta\",\n",
        "            \"Chopra\", \"Reddy\", \"Bose\", \"Verma\", \"Kapoor\", \"Anand\", \"Khanna\", \"Chowdhury\",\n",
        "            \"Mukherjee\", \"Chatterjee\", \"Agarwal\", \"Mukhopadhyay\", \"Das\", \"Banerjee\", \"Desai\", \"Malhotra\"\n",
        "        }\n",
        "\n",
        "        # Add common Muslim names\n",
        "        muslim_names = {\n",
        "            \"Mohammed\", \"Muhammad\", \"Ahmad\", \"Mahmoud\", \"Abdullah\", \"Ali\", \"Omar\", \"Hassan\",\n",
        "            \"Hussein\", \"Ibrahim\", \"Khalid\", \"Tariq\", \"Waleed\", \"Yusuf\", \"Zaid\", \"Bilal\",\n",
        "            \"Hamza\", \"Mustafa\", \"Samir\", \"Rayan\", \"Malik\", \"Jamal\", \"Karim\", \"Amir\",\n",
        "            \"Fatima\", \"Aisha\", \"Maryam\", \"Zainab\", \"Layla\", \"Noor\", \"Hana\", \"Samira\",\n",
        "            \"Amina\", \"Leila\", \"Salma\", \"Yasmin\", \"Zahra\", \"Rania\", \"Sara\", \"Lina\",\n",
        "            \"Noura\", \"Khadija\", \"Sana\", \"Farah\", \"Iman\", \"Nadia\", \"Aliyah\", \"Dalia\",\n",
        "            \"Khan\", \"Rahman\", \"Ahmed\", \"Pasha\", \"Malik\", \"Syed\", \"Qureshi\", \"Aziz\",\n",
        "            \"Hussain\", \"Farooq\", \"Javed\", \"Iqbal\", \"Abbas\", \"Rashid\", \"Mirza\", \"Asif\"\n",
        "        }\n",
        "\n",
        "        # Combine all name sets\n",
        "        self.first_names = male_names.union(female_names).union(indian_names).union(muslim_names)\n",
        "\n",
        "        # Add some common name prefixes and suffixes for various cultures\n",
        "        self.name_prefixes = {\n",
        "            \"Al\", \"El\", \"Abd\", \"Abdul\", \"Bin\", \"Ibn\", \"Abu\",\n",
        "            \"Mc\", \"Mac\", \"De\", \"Van\", \"Von\", \"O'\", \"St.\", \"San\"\n",
        "        }\n",
        "\n",
        "        self.name_suffixes = {\n",
        "            \"Jr\", \"Jr.\", \"Sr\", \"Sr.\", \"II\", \"III\", \"IV\",\n",
        "            \"Khan\", \"Lal\", \"Devi\", \"Wala\", \"Bhai\", \"Ji\", \"Babu\"\n",
        "        }\n",
        "\n",
        "        # We'll use a probability threshold for accepting names\n",
        "        self.name_probability = {}\n",
        "        for name in self.first_names:\n",
        "            self.name_probability[name] = 1.0\n",
        "            # Also add lowercase version with slightly lower probability\n",
        "            self.name_probability[name.lower()] = 0.9\n",
        "\n",
        "    def is_likely_human_name(self, word):\n",
        "        \"\"\"Determine if a word is likely to be a human name with multicultural awareness\"\"\"\n",
        "        # If word is in our known name list, it's highly likely\n",
        "        if word in self.first_names:\n",
        "            return True\n",
        "\n",
        "        # Check for name prefixes and suffixes that indicate a name\n",
        "        for prefix in self.name_prefixes:\n",
        "            if word.startswith(prefix) and len(word) > len(prefix) + 1:\n",
        "                return True\n",
        "\n",
        "        for suffix in self.name_suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "                return True\n",
        "\n",
        "        # Check if it's a common non-name word\n",
        "        if word.lower() in self.stopwords:\n",
        "            return False\n",
        "\n",
        "        if word in self.common_capitalized:\n",
        "            return False\n",
        "\n",
        "        # Cultural name patterns\n",
        "        # Check for common Indian and Muslim name patterns\n",
        "\n",
        "        # Common Indian name endings\n",
        "        indian_suffixes = ['ji', 'nath', 'raj', 'dev', 'pal', 'kar', 'wati', 'deep', 'preet', 'jeet']\n",
        "        for suffix in indian_suffixes:\n",
        "            if word.lower().endswith(suffix) and len(word) > len(suffix) + 2:\n",
        "                return True\n",
        "\n",
        "        # Common Muslim name patterns\n",
        "        muslim_prefixes = ['al-', 'abdul', 'ibn', 'abu', 'bin', 'mohammad', 'muhammad']\n",
        "        for prefix in muslim_prefixes:\n",
        "            if word.lower().startswith(prefix) and len(word) > len(prefix) + 2:\n",
        "                return True\n",
        "\n",
        "        # For multicultural names, be less strict about excluding based on English dictionary\n",
        "        # Only exclude if it's a very common English word AND short (as common short words are\n",
        "        # unlikely to be names across cultures)\n",
        "        if len(word) <= 4 and word.lower() in self.english_words and word.lower() not in self.name_probability:\n",
        "            return False\n",
        "\n",
        "        # Additional checks\n",
        "        # Names typically don't have numbers\n",
        "        if any(c.isdigit() for c in word):\n",
        "            return False\n",
        "\n",
        "        # Multicultural names may have special characters, but generally alphanumeric with some exceptions\n",
        "        allowed_chars = \"-'., \"\n",
        "        if any(c not in allowed_chars and not c.isalpha() for c in word):\n",
        "            return False\n",
        "\n",
        "        # Most names have a reasonable length\n",
        "        if len(word) < 2 or len(word) > 30:  # Increased max length for compound names\n",
        "            return False\n",
        "\n",
        "        # This is a heuristic - names typically don't end in certain suffixes\n",
        "        # But be careful with multicultural names that might use these patterns\n",
        "        non_name_suffixes = ['ing', 'ed', 'ly', 'tion', 'ment', 'ness', 'ity', 'ism']\n",
        "        if (word.lower().endswith(tuple(non_name_suffixes)) and\n",
        "            len(word) > 5 and  # Only apply to longer words\n",
        "            not any(word.lower().endswith(cultural_suffix) for cultural_suffix in indian_suffixes)):\n",
        "            return False\n",
        "\n",
        "        # If the word passed all filters and is capitalized, it might be a name\n",
        "        # For multicultural names, capitalization is important but not definitive\n",
        "        if word[0].isupper():\n",
        "            return True\n",
        "\n",
        "        # For non-capitalized words, check if they match cultural patterns\n",
        "        # This helps with names that might be written in lowercase\n",
        "        return any(word.lower().startswith(p) for p in muslim_prefixes) or any(word.lower().endswith(s) for s in indian_suffixes)\n",
        "\n",
        "    def verify_person_entity(self, ent_text):\n",
        "        \"\"\"Verify a spaCy PERSON entity is likely a real name with multicultural awareness\"\"\"\n",
        "        parts = ent_text.split()\n",
        "\n",
        "        # Single-word name check\n",
        "        if len(parts) == 1:\n",
        "            return self.is_likely_human_name(parts[0])\n",
        "\n",
        "        # For multi-word names, check if at least one part is in our name dataset\n",
        "        for part in parts:\n",
        "            if part in self.first_names:\n",
        "                return True\n",
        "\n",
        "        # Handle compound names with hyphens or special characters\n",
        "        # Example: \"Abdul-Rahman\" or \"Mohammed Al-Fayed\"\n",
        "        compound_parts = []\n",
        "        for part in parts:\n",
        "            compound_parts.extend([p for p in re.split(r'[-\\s]', part) if p])\n",
        "\n",
        "        for part in compound_parts:\n",
        "            if part in self.first_names:\n",
        "                return True\n",
        "\n",
        "        # Check for cultural name patterns\n",
        "        # Handle common Indian name patterns (e.g., first name + father's name or place name)\n",
        "        if len(parts) >= 2:\n",
        "            # Check if any part matches name patterns\n",
        "            indian_suffixes = ['ji', 'nath', 'raj', 'dev', 'pal', 'kar', 'wati', 'deep', 'preet', 'jeet']\n",
        "            muslim_prefixes = ['al-', 'abdul', 'ibn', 'abu', 'bin', 'mohammad', 'muhammad']\n",
        "\n",
        "            for part in parts:\n",
        "                # Check for Indian name patterns\n",
        "                if any(part.lower().endswith(suffix) for suffix in indian_suffixes):\n",
        "                    return True\n",
        "\n",
        "                # Check for Muslim name patterns\n",
        "                if any(part.lower().startswith(prefix) for prefix in muslim_prefixes):\n",
        "                    return True\n",
        "\n",
        "        # If no parts match known patterns, check general name patterns\n",
        "        # For multicultural names, be more lenient\n",
        "        if len(parts) == 2 or len(parts) == 3:\n",
        "            # Check if all parts are capitalized (basic name pattern across cultures)\n",
        "            if all(p[0].isupper() for p in parts):\n",
        "                # Check that parts don't look like obvious verbs or common nouns\n",
        "                verb_suffixes = ['ing', 'ed']\n",
        "                if all(not any(p.lower().endswith(s) for s in verb_suffixes) for p in parts):\n",
        "                    return True\n",
        "\n",
        "        # For 3+ part names (common in some cultures), check capitalization\n",
        "        if len(parts) >= 3:\n",
        "            if all(p[0].isupper() for p in parts):\n",
        "                return True\n",
        "\n",
        "        # Default to spaCy's judgment with moderate confidence for multicultural names\n",
        "        # SpaCy has been trained on diverse datasets and may recognize patterns we don't explicitly check\n",
        "        return len(parts) >= 2 and all(p[0].isupper() for p in parts)\n",
        "\n",
        "    def add_name(self, name, confidence=1.0):\n",
        "        \"\"\"Add a name to be processed, with confidence filter\"\"\"\n",
        "        # Skip names with low confidence or very short names\n",
        "        if confidence < 0.5 or not name or len(name) < 2:\n",
        "            return\n",
        "\n",
        "        # Check if this looks like a name\n",
        "        if not any(part in self.first_names for part in name.split()):\n",
        "            # If no part of the name is in our known names list,\n",
        "            # require higher confidence\n",
        "            if confidence < 0.8:\n",
        "                return\n",
        "\n",
        "        # Get the metaphone code\n",
        "        try:\n",
        "            metaphone_code = fuzzy.DMetaphone()(name)[0]\n",
        "            if not metaphone_code:\n",
        "                return\n",
        "\n",
        "            # Convert to string for consistency\n",
        "            metaphone_code = metaphone_code.decode('utf-8') if isinstance(metaphone_code, bytes) else metaphone_code\n",
        "\n",
        "            # If this is a new metaphone code\n",
        "            if metaphone_code not in self.metaphone_mapping:\n",
        "                self.metaphone_mapping[metaphone_code] = name\n",
        "                self.name_groups[metaphone_code] = {name}\n",
        "            else:\n",
        "                # Use Levenshtein distance to decide if this is truly the same name\n",
        "                standard_name = self.metaphone_mapping[metaphone_code]\n",
        "                distance = Levenshtein.distance(name.lower(), standard_name.lower())\n",
        "\n",
        "                # If very similar, add to existing group\n",
        "                if distance <= min(len(name), len(standard_name)) * 0.4:  # 40% threshold\n",
        "                    self.name_groups[metaphone_code].add(name)\n",
        "\n",
        "                    # Use the shorter name as standard if it's not just an initial\n",
        "                    if len(name) < len(standard_name) and len(name) > 2:\n",
        "                        self.metaphone_mapping[metaphone_code] = name\n",
        "\n",
        "                    # If current name is longer and has capital letters (likely more formal)\n",
        "                    elif len(name) > len(standard_name) and sum(1 for c in name if c.isupper()) > sum(1 for c in standard_name if c.isupper()):\n",
        "                        self.metaphone_mapping[metaphone_code] = name\n",
        "\n",
        "                # If this is a different name with same metaphone code\n",
        "                else:\n",
        "                    # Generate a unique key by appending a number\n",
        "                    new_key = f\"{metaphone_code}_{len([k for k in self.metaphone_mapping if k.startswith(metaphone_code)])}\"\n",
        "                    self.metaphone_mapping[new_key] = name\n",
        "                    self.name_groups[new_key] = {name}\n",
        "\n",
        "            # Map this variant to the standard name\n",
        "            standard_name = self.metaphone_mapping[metaphone_code]\n",
        "            self.name_mapping[name] = standard_name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing name '{name}': {e}\")\n",
        "\n",
        "    def extract_names_from_text(self, text):\n",
        "        \"\"\"Extract potential names from text with improved filtering\"\"\"\n",
        "        # Process with spaCy to find named entities\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Store context information\n",
        "        name_indicators = [\"mr\", \"mr.\", \"ms\", \"ms.\", \"mrs\", \"mrs.\", \"dr\", \"dr.\",\n",
        "                          \"professor\", \"prof\", \"prof.\", \"sir\", \"madam\", \"miss\"]\n",
        "\n",
        "        # Dictionary to store names with their confidence scores\n",
        "        extracted_names = {}\n",
        "\n",
        "        # Extract names from entities with verification\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                # Check if this is likely a real name\n",
        "                if self.verify_person_entity(ent.text):\n",
        "                    # Check if there are name indicators before this entity\n",
        "                    confidence = 0.9  # Base confidence for spaCy PERSON entities\n",
        "\n",
        "                    # Look for name indicators in the previous tokens\n",
        "                    for token in doc:\n",
        "                        if token.text.lower() in name_indicators and token.i < ent.start and token.i >= ent.start - 3:\n",
        "                            confidence = 1.0  # Highest confidence with name indicators\n",
        "\n",
        "                    # Add the full name with high confidence\n",
        "                    if len(ent.text.split()) > 1:\n",
        "                        extracted_names[ent.text] = confidence\n",
        "\n",
        "                    # Add individual parts with slightly lower confidence\n",
        "                    for name_part in ent.text.split():\n",
        "                        if name_part[0].isupper() and len(name_part) > 1:\n",
        "                            # Only add individual parts that are likely names\n",
        "                            if self.is_likely_human_name(name_part):\n",
        "                                extracted_names[name_part] = confidence - 0.1\n",
        "\n",
        "        # Additional pattern-based name extraction for cases spaCy might miss\n",
        "        words = word_tokenize(text)\n",
        "        for i, word in enumerate(words):\n",
        "            # Skip words already processed\n",
        "            if word in extracted_names:\n",
        "                continue\n",
        "\n",
        "            # Check for name indicators followed by capitalized words\n",
        "            if i > 0 and words[i-1].lower().rstrip('.') in [ind.rstrip('.') for ind in name_indicators]:\n",
        "                if word[0].isupper() and self.is_likely_human_name(word):\n",
        "                    extracted_names[word] = 0.95\n",
        "\n",
        "                    # Try to get the full name if it's a multi-word name\n",
        "                    if i < len(words) - 1 and words[i+1][0].isupper() and self.is_likely_human_name(words[i+1]):\n",
        "                        full_name = f\"{word} {words[i+1]}\"\n",
        "                        extracted_names[full_name] = 0.98\n",
        "\n",
        "        # Add all extracted names with their confidence values\n",
        "        for name, confidence in extracted_names.items():\n",
        "            self.add_name(name, confidence)\n",
        "\n",
        "    def standardize_name(self, name):\n",
        "        \"\"\"Return the standardized version of a name\"\"\"\n",
        "        if not name or len(name) < 2:\n",
        "            return name\n",
        "\n",
        "        # First verify this is likely a name\n",
        "        if not self.is_likely_human_name(name.split()[0]):\n",
        "            return name  # Return unchanged if not likely a name\n",
        "\n",
        "        # If we've seen this name before, return its standard form\n",
        "        if name in self.name_mapping:\n",
        "            return self.name_mapping[name]\n",
        "\n",
        "        # Try to match it by metaphone\n",
        "        try:\n",
        "            metaphone_code = fuzzy.DMetaphone()(name)[0]\n",
        "            if metaphone_code:\n",
        "                metaphone_code = metaphone_code.decode('utf-8') if isinstance(metaphone_code, bytes) else metaphone_code\n",
        "                if metaphone_code in self.metaphone_mapping:\n",
        "                    standard_name = self.metaphone_mapping[metaphone_code]\n",
        "                    self.name_mapping[name] = standard_name\n",
        "                    return standard_name\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # If no match found, add it as a new name and return itself\n",
        "        if self.is_likely_human_name(name.split()[0]):\n",
        "            self.add_name(name, 0.8)  # Add with moderate confidence\n",
        "        return name\n",
        "\n",
        "    def standardize_names_in_text(self, text):\n",
        "        \"\"\"Find and standardize all names in a text\"\"\"\n",
        "        # First extract potential names\n",
        "        self.extract_names_from_text(text)\n",
        "\n",
        "        # Sort names by length (longest first) to handle substring matches properly\n",
        "        names_by_length = sorted(self.name_mapping.keys(), key=len, reverse=True)\n",
        "\n",
        "        # Replace all name occurrences\n",
        "        for name in names_by_length:\n",
        "            # Create a regex pattern that handles word boundaries\n",
        "            pattern = r'\\b' + re.escape(name) + r'\\b'\n",
        "            text = re.sub(pattern, self.name_mapping[name], text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def get_name_variants(self):\n",
        "        \"\"\"Return all name variants and their standardized forms\"\"\"\n",
        "        return self.name_mapping\n",
        "\n",
        "    def get_name_groups(self):\n",
        "        \"\"\"Return groups of names that were matched together\"\"\"\n",
        "        return self.name_groups\n",
        "\n",
        "\n",
        "class AudioProcessor:\n",
        "    def __init__(self, model_size=\"small\"):\n",
        "        with st.spinner('Loading Whisper model...'):\n",
        "            self.model = whisper.load_model(model_size)\n",
        "\n",
        "    def get_audio_duration(self, audio_path: str) -> float:\n",
        "        \"\"\"Get the duration of an audio file in seconds\"\"\"\n",
        "        try:\n",
        "            duration = librosa.get_duration(path=audio_path)\n",
        "            return duration\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Could not determine audio duration: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def format_duration(self, duration_seconds: float) -> str:\n",
        "        \"\"\"Format duration in seconds to HH:MM:SS format\"\"\"\n",
        "        td = timedelta(seconds=int(duration_seconds))\n",
        "        hours, remainder = divmod(td.seconds, 3600)\n",
        "        minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "        if hours > 0:\n",
        "            return f\"{hours} hour{'s' if hours > 1 else ''}, {minutes} minute{'s' if minutes > 1 else ''}\"\n",
        "        else:\n",
        "            return f\"{minutes} minute{'s' if minutes > 1 else ''}, {seconds} second{'s' if seconds > 1 else ''}\"\n",
        "\n",
        "    def transcribe(self, audio_path: str) -> str:\n",
        "        \"\"\"Transcribe audio file using Whisper\"\"\"\n",
        "        try:\n",
        "            result = self.model.transcribe(audio_path, task=\"translate\", verbose=False)  # This will translate to English\n",
        "            return result[\"text\"]\n",
        "        finally:\n",
        "            # Clear GPU memory after transcription\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PreprocessingConfig:\n",
        "    remove_timestamps: bool = True\n",
        "    remove_fillers: bool = True\n",
        "    fix_contractions: bool = True\n",
        "    remove_duplicates: bool = True\n",
        "    fix_punctuation: bool = True\n",
        "    segment_sentences: bool = True\n",
        "    remove_stopwords: bool = False\n",
        "    chunk_size: int = 512\n",
        "    chunk_overlap: int = 50\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, config: PreprocessingConfig = PreprocessingConfig()):\n",
        "        self.config = config\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.name_standardizer = NameStandardizer()\n",
        "\n",
        "        self.fillers = [\n",
        "            \"um\", \"uh\", \"like\", \"you know\", \"i mean\",\n",
        "            \"so\", \"basically\", \"actually\", \"literally\",\n",
        "            \"sort of\", \"kind of\", \"well\"\n",
        "        ]\n",
        "\n",
        "        self.contractions = {\n",
        "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "            \"i'm\": \"i am\", \"i've\": \"i have\", \"you're\": \"you are\",\n",
        "            \"you've\": \"you have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "            \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "            \"it's\": \"it is\", \"that's\": \"that is\", \"what's\": \"what is\",\n",
        "            \"let's\": \"let us\", \"who's\": \"who is\"\n",
        "        }\n",
        "\n",
        "        # Compile regex patterns\n",
        "        self.timestamp_pattern = re.compile(r'\\[\\d{2}:\\d{2}:\\d{2}\\]')\n",
        "        self.filler_pattern = re.compile(r'\\b(?:' + '|'.join(self.fillers) + r')\\b', re.IGNORECASE)\n",
        "        self.duplicate_pattern = re.compile(r'\\b(\\w+)(?:\\s+\\1)+\\b', re.IGNORECASE)\n",
        "        self.punctuation_pattern = re.compile(r'([.,!?])([^\\s])')\n",
        "        self.spaces_pattern = re.compile(r'\\s+')\n",
        "        self.quotes_pattern = re.compile(r'\"\\s*([^\"]*?)\\s*\"')\n",
        "\n",
        "    def preprocess(self, text: str) -> str:\n",
        "        \"\"\"Apply all preprocessing steps based on configuration\"\"\"\n",
        "        if self.config.remove_timestamps:\n",
        "            text = self.timestamp_pattern.sub('', text)\n",
        "        if self.config.remove_fillers:\n",
        "            text = self.filler_pattern.sub('', text)\n",
        "        if self.config.fix_contractions:\n",
        "            for contraction, expansion in self.contractions.items():\n",
        "                text = re.sub(rf'\\b{contraction}\\b', expansion, text, flags=re.IGNORECASE)\n",
        "        if self.config.remove_duplicates:\n",
        "            text = self.duplicate_pattern.sub(r'\\1', text)\n",
        "        if self.config.fix_punctuation:\n",
        "            text = self.punctuation_pattern.sub(r'\\1 \\2', text)\n",
        "            text = self.spaces_pattern.sub(' ', text)\n",
        "            text = self.quotes_pattern.sub(r'\"\\1\"', text)\n",
        "            text = text.strip()\n",
        "\n",
        "        # Extract and standardize names before segmenting\n",
        "        self.name_standardizer.extract_names_from_text(text)\n",
        "        text = self.name_standardizer.standardize_names_in_text(text)\n",
        "\n",
        "        if self.config.segment_sentences:\n",
        "            text = ' '.join(sent_tokenize(text))\n",
        "        if self.config.remove_stopwords:\n",
        "            words = text.split()\n",
        "            text = ' '.join(word for word in words if word.lower() not in self.stop_words)\n",
        "        return text\n",
        "\n",
        "    def split_into_chunks(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), self.config.chunk_size - self.config.chunk_overlap):\n",
        "            chunk = ' '.join(words[i:i + self.config.chunk_size])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def get_name_standardization_report(self):\n",
        "        \"\"\"Get a report of name standardization\"\"\"\n",
        "        name_groups = self.name_standardizer.get_name_groups()\n",
        "        report = []\n",
        "\n",
        "        for metaphone, names in name_groups.items():\n",
        "            if len(names) > 1:  # Only show groups with variants\n",
        "                standard_name = self.name_standardizer.metaphone_mapping.get(metaphone, list(names)[0])\n",
        "                variants = [name for name in names if name != standard_name]\n",
        "                if variants:  # Only report if there are actual variants\n",
        "                    report.append(f\"Standardized '{standard_name}' from variants: {', '.join(variants)}\")\n",
        "\n",
        "        return report\n",
        "\n",
        "\n",
        "class SummaryPostProcessor:\n",
        "    def __init__(self):\n",
        "        with st.spinner('Loading language model...'):\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def fix_sentence_boundaries(self, text: str) -> str:\n",
        "        \"\"\"Fix sentence boundaries and capitalization\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        sentences = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            fixed_sent = sent.text.strip()\n",
        "            if fixed_sent:\n",
        "                fixed_sent = fixed_sent[0].upper() + fixed_sent[1:]\n",
        "                if not fixed_sent[-1] in {'.', '!', '?'}:\n",
        "                    fixed_sent += '.'\n",
        "                sentences.append(fixed_sent)\n",
        "\n",
        "        return ' '.join(sentences)\n",
        "\n",
        "    def remove_incomplete_sentences(self, text: str) -> str:\n",
        "        \"\"\"Remove fragments and incomplete sentences\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        complete_sentences = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            has_subject = False\n",
        "            has_verb = False\n",
        "\n",
        "            for token in sent:\n",
        "                if token.dep_ in {'nsubj', 'nsubjpass'}:\n",
        "                    has_subject = True\n",
        "                if token.pos_ == 'VERB':\n",
        "                    has_verb = True\n",
        "\n",
        "            if has_subject and has_verb:\n",
        "                complete_sentences.append(sent.text.strip())\n",
        "\n",
        "        return ' '.join(complete_sentences)\n",
        "\n",
        "    def fix_conjunctions(self, text: str) -> str:\n",
        "        \"\"\"Fix common conjunction issues and improve flow\"\"\"\n",
        "        text = re.sub(r'\\band\\s+and\\b', 'and', text)\n",
        "        text = re.sub(r'\\bbut\\s+but\\b', 'but', text)\n",
        "        text = re.sub(r'\\s+but\\s+', ', but ', text)\n",
        "        text = re.sub(r'\\s+however\\s+', ', however, ', text)\n",
        "        return text\n",
        "\n",
        "    def fix_list_formatting(self, sentences: List[str]) -> List[str]:\n",
        "        \"\"\"Improve formatting of list items\"\"\"\n",
        "        formatted_sentences = []\n",
        "        for sent in sentences:\n",
        "            sent = re.sub(r'\\s+\\s+', '. ', sent)\n",
        "            sent = re.sub(r'^\\s*\\s*', '', sent)\n",
        "            sent = re.sub(r'^\\d+\\.\\s*([a-z])', lambda m: f\"{m.group(1).upper()}\", sent)\n",
        "            formatted_sentences.append(sent)\n",
        "        return formatted_sentences\n",
        "\n",
        "    def postprocess_summary(self, text: str) -> str:\n",
        "        \"\"\"Apply all post-processing steps\"\"\"\n",
        "        text = self.fix_sentence_boundaries(text)\n",
        "        text = self.remove_incomplete_sentences(text)\n",
        "        text = self.fix_conjunctions(text)\n",
        "        sentences = [s.strip() for s in re.split('[.!?]', text) if s.strip()]\n",
        "        formatted_sentences = self.fix_list_formatting(sentences)\n",
        "        return '. '.join(formatted_sentences) + '.'\n",
        "\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        with st.spinner('Loading BART summarization model...'):\n",
        "            self.summarizer = pipeline(\n",
        "                \"summarization\",\n",
        "                model=\"facebook/bart-large-cnn\",\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "            )\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
        "            self.post_processor = SummaryPostProcessor()\n",
        "\n",
        "    def process_chunks(self, chunks: List[str], progress_bar) -> Dict[str, List[str]]:\n",
        "        \"\"\"Process chunks and group by topics with post-processing\"\"\"\n",
        "        grouped_chunks = defaultdict(list)\n",
        "        total_chunks = len(chunks)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            summary = self.summarizer(chunk, max_length=130, min_length=30)[0]['summary_text']\n",
        "            processed_summary = self.post_processor.postprocess_summary(summary)\n",
        "            topic = self.identify_topics(chunk)\n",
        "            grouped_chunks[topic].append(processed_summary)\n",
        "\n",
        "            # Update progress\n",
        "            progress = (i + 1) / total_chunks\n",
        "            progress_bar.progress(progress)\n",
        "\n",
        "            if len(grouped_chunks) % 5 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        return grouped_chunks\n",
        "\n",
        "    def identify_topics(self, text: str) -> str:\n",
        "        \"\"\"Identify the main topic of a text segment\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        topic_keywords = {\n",
        "    #  Business & Corporate\n",
        "    'Company Overview': ['company', 'organization', 'mission', 'vision', 'values'],\n",
        "    'Business Strategy & Management': ['strategy', 'leadership', 'management', 'growth', 'operations', 'business model', 'entrepreneurship', 'stakeholders'],\n",
        "    'Finance & Investment': ['finance', 'investment', 'funding', 'profit', 'loss', 'budget', 'capital', 'valuation', 'stocks', 'trading'],\n",
        "    'Marketing & Branding': ['marketing', 'branding', 'advertising', 'SEO', 'social media', 'promotion', 'public relations'],\n",
        "    'Sales & Customer Relationship': [\n",
        "        'sales', 'CRM', 'lead generation', 'customer service', 'retention', 'pricing strategy',\n",
        "        'follow-ups', 'quotations', 'lead status', 'client engagement', 'stakeholder meeting',\n",
        "        'customer pipeline', 'business development'\n",
        "    ],\n",
        "    'E-Commerce & Retail': ['e-commerce', 'retail', 'supply chain', 'inventory', 'logistics', 'warehousing'],\n",
        "    'Corporate Strategy & Financials': ['investors', 'corporate', 'supply chain', 'mergers', 'acquisitions'],\n",
        "    'Market Expansion & Globalization': ['market', 'expansion', 'international trade', 'emerging markets'],\n",
        "    'Client Engagement & Sales Strategy': [\n",
        "        'client', 'POC', 'demo', 'pipeline', 'licensing', 'collateral', 'roadmap', 'proposal',\n",
        "        'investment roadmap', 'business negotiations', 'partner meetings'\n",
        "    ],\n",
        "    'Project & Report Management': [\n",
        "        'report format', 'summary report', 'documentation', 'findings', 'meeting notes',\n",
        "        'proposal customization', 'contract discussions'\n",
        "    ],\n",
        "\n",
        "    #  Technology & AI\n",
        "    'Artificial Intelligence & Automation': [\n",
        "        'AI', 'machine learning', 'deep learning', 'automation', 'data analytics', 'neural networks',\n",
        "        'ML model', 'data-driven insights', 'intelligent automation', 'predictive analytics'\n",
        "    ],\n",
        "    'Cybersecurity & Risk Management': [\n",
        "        'cybersecurity', 'HTTPS', 'encryption', 'TLS', 'attack', 'authorization', 'security',\n",
        "        'penetration testing', 'ransomware', 'risk register', 'remediation', 'audit', 'SOC',\n",
        "        'security compliance', 'risk-based approach', 'security audits', 'VAPT'\n",
        "    ],\n",
        "    'Software Development & IT': [\n",
        "        'software', 'coding', 'cloud computing', 'DevOps', 'databases', 'full stack development',\n",
        "        'API', 'staging', 'production', 'server', 'software customization', 'IT infrastructure'\n",
        "    ],\n",
        "    'Testing & Evaluation': ['testing', 'scoring', 'quantitative', 'qualitative', 'user testing', 'penetration testing'],\n",
        "    'Blockchain & Cryptocurrency': ['blockchain', 'cryptocurrency', 'bitcoin', 'ethereum', 'NFTs', 'decentralization'],\n",
        "    'Internet & Digital Trends': ['internet', 'IoT', 'web development', '5G', 'cloud services', 'big data'],\n",
        "\n",
        "    #  Science & Engineering\n",
        "    'Healthcare & Medicine': [\n",
        "        'healthcare', 'medicine', 'hospital', 'surgery', 'doctor', 'medical research',\n",
        "        'healthcare compliance', 'HIPAA', 'patient data security', 'healthcare AI solutions'\n",
        "    ],\n",
        "    'Biotechnology & Pharmaceuticals': ['biotech', 'pharmaceuticals', 'drug discovery', 'genomics', 'clinical trials'],\n",
        "    'Engineering & Innovation': ['engineering', 'mechanical', 'electrical', 'civil', 'robotics', 'nanotechnology'],\n",
        "    'Environmental Science & Sustainability': ['sustainability', 'climate change', 'carbon footprint', 'renewable energy'],\n",
        "    'Energy & Emissions': ['energy', 'renewable', 'non-renewable', 'solar', 'wind', 'tidal', 'carbon emissions', 'power plants'],\n",
        "    'Space Exploration & Astronomy': ['astronomy', 'NASA', 'Mars', 'black holes', 'cosmology', 'satellites'],\n",
        "    'Physics & Chemistry': ['quantum mechanics', 'thermodynamics', 'chemical reactions', 'material science'],\n",
        "\n",
        "    #  Education & Learning\n",
        "    'Academic Research & Education': [\n",
        "        'education', 'learning', 'teaching', 'university', 'school', 'curriculum',\n",
        "        'educational technology', 'e-learning platforms'\n",
        "    ],\n",
        "    'Online Learning & EdTech': ['e-learning', 'MOOCs', 'virtual classroom', 'online courses', 'EdTech'],\n",
        "\n",
        "    #  Government & Policy\n",
        "    'Regulations & Compliance': [\n",
        "        'regulation', 'compliance', 'laws', 'policy', 'audit', 'standards',\n",
        "        'cybersecurity compliance', 'government regulations', 'data privacy laws'\n",
        "    ],\n",
        "    'Politics & International Relations': ['politics', 'government', 'elections', 'diplomacy', 'foreign policy'],\n",
        "    'Government Policies & Global Agreements': ['United Nations', 'developing countries', 'global cooperation'],\n",
        "\n",
        "    #  Society & Ethics\n",
        "    'Diversity & Inclusion': ['diversity', 'inclusion', 'gender equality', 'racism', 'social justice'],\n",
        "    'Philosophy & Ethics': ['philosophy', 'morality', 'ethics', 'values', 'debates'],\n",
        "    'Legal & Criminal Justice': ['law', 'justice', 'court', 'crime', 'legal system'],\n",
        "\n",
        "    #  Personal Development & Lifestyle\n",
        "    'Self-Improvement & Productivity': ['self-improvement', 'motivation', 'goal setting', 'habits', 'efficiency', 'workflow'],\n",
        "    'Fitness & Wellness': ['fitness', 'exercise', 'yoga', 'meditation', 'nutrition'],\n",
        "    'Travel & Adventure': ['travel', 'tourism', 'adventure', 'hotels'],\n",
        "\n",
        "    #  Entertainment & Media\n",
        "    'Movies & TV Shows': ['movies', 'TV shows', 'cinema', 'streaming', 'actors', 'directors'],\n",
        "    'Music & Performing Arts': ['music', 'concerts', 'albums', 'orchestra', 'theater'],\n",
        "    'Gaming & Esports': ['gaming', 'esports', 'video games', 'VR', 'tournaments'],\n",
        "\n",
        "    #  Transportation & Automotive\n",
        "    'Automobile & Electric Vehicles': ['cars', 'EVs', 'batteries', 'hybrid cars', 'self-driving'],\n",
        "    'Aviation & Aerospace': ['aviation', 'airlines', 'drones', 'rockets'],\n",
        "    'Maritime & Shipping': ['shipping', 'cargo', 'logistics'],\n",
        "\n",
        "    #  Home & Lifestyle\n",
        "    'Interior Design & Home Improvement': ['interior design', 'renovation', 'home decor'],\n",
        "    'Food & Culinary Arts': ['cooking', 'recipes', 'restaurants'],\n",
        "    'Fashion & Beauty': ['fashion', 'clothing', 'skincare', 'makeup'],\n",
        "\n",
        "    #  Psychology & Behavior\n",
        "    'Psychology & Mental Health': ['psychology', 'behavior', 'cognition', 'therapy'],\n",
        "    'Neuroscience & Cognitive Science': ['neuroscience', 'brain', 'learning'],\n",
        "\n",
        "    #  Miscellaneous\n",
        "    'Mythology & Folklore': ['myths', 'legends', 'folklore'],\n",
        "    'History & Archaeology': ['history', 'archaeology', 'ancient civilizations'],\n",
        "    'Mystery & Conspiracy Theories': ['conspiracy', 'mystery', 'UFOs', 'secret societies']\n",
        "}\n",
        "\n",
        "        topic_scores = defaultdict(int)\n",
        "        for token in doc:\n",
        "            for topic, keywords in topic_keywords.items():\n",
        "                if token.text.lower() in keywords:\n",
        "                    topic_scores[topic] += 1\n",
        "\n",
        "        return max(topic_scores.items(), key=lambda x: x[1])[0] if topic_scores else 'General Discussion'\n",
        "\n",
        "    def remove_redundancies(self, chunks: List[str]) -> List[str]:\n",
        "        \"\"\"Remove redundant content using TF-IDF and cosine similarity\"\"\"\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "        similarities = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "        unique_chunks = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            is_redundant = False\n",
        "            for j in range(i):\n",
        "                if similarities[i][j] > 0.7:\n",
        "                    is_redundant = True\n",
        "                    break\n",
        "            if not is_redundant:\n",
        "                unique_chunks.append(chunk)\n",
        "\n",
        "        return unique_chunks\n",
        "\n",
        "    def format_markdown(self, grouped_chunks: Dict[str, List[str]], meeting_duration: str = None, name_standardization_report: List[str] = None) -> str:\n",
        "        \"\"\"Format the grouped chunks into a markdown document with meeting duration\"\"\"\n",
        "        md_content = [\"# Meeting Summary\\n\"]\n",
        "\n",
        "        # Add meeting duration if available\n",
        "        if meeting_duration:\n",
        "            md_content.append(f\"**Meeting Duration:** {meeting_duration}\\n\")\n",
        "\n",
        "        # Add overview section\n",
        "        md_content.append(\"## Overview\")\n",
        "        topics = list(grouped_chunks.keys())\n",
        "        md_content.append(\"This meeting covered the following topics:\")\n",
        "        for topic in topics:\n",
        "            if topic:\n",
        "                md_content.append(f\"- {topic}\")\n",
        "        md_content.append(\"\")\n",
        "\n",
        "        # Add detailed sections\n",
        "        for topic, chunks in grouped_chunks.items():\n",
        "            if not topic:\n",
        "                continue\n",
        "\n",
        "            md_content.append(f\"## {topic}\")\n",
        "            topic_text = ' '.join(chunks)\n",
        "            final_summary = self.post_processor.postprocess_summary(topic_text)\n",
        "            sentences = sent_tokenize(final_summary)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip():\n",
        "                    md_content.append(f\"- {sentence.strip()}\")\n",
        "            md_content.append(\"\")\n",
        "\n",
        "        md_content.append(\"## Next Steps\")\n",
        "        md_content.append(\"- [ ] Review shared materials\")\n",
        "        md_content.append(\"- [ ] Confirm availability for the next meeting\")\n",
        "        md_content.append(\"- [ ] Share summary with stakeholders\\n\")\n",
        "\n",
        "        return \"\\n\".join(md_content)\n",
        "\n",
        "\n",
        "class PDFGenerator:\n",
        "    def __init__(self):\n",
        "        # Initialize styles for reportlab\n",
        "        self.styles = getSampleStyleSheet()\n",
        "        # Create custom styles without using .add() to avoid conflicts\n",
        "        self.heading1_style = ParagraphStyle(\n",
        "            name='CustomHeading1',\n",
        "            parent=self.styles['Heading1'],\n",
        "            fontSize=24,\n",
        "            spaceAfter=12,\n",
        "        )\n",
        "        self.heading2_style = ParagraphStyle(\n",
        "            name='CustomHeading2',\n",
        "            parent=self.styles['Heading2'],\n",
        "            fontSize=18,\n",
        "            spaceAfter=10,\n",
        "        )\n",
        "        self.bullet_style = ParagraphStyle(\n",
        "            name='CustomBullet',\n",
        "            parent=self.styles['Normal'],\n",
        "            leftIndent=20,\n",
        "            firstLineIndent=0,\n",
        "            spaceBefore=2,\n",
        "            bulletIndent=10,\n",
        "        )\n",
        "\n",
        "    def sanitize_text(self, text):\n",
        "        \"\"\"Replace problematic Unicode characters with ASCII alternatives\"\"\"\n",
        "        replacements = {\n",
        "            '': '*',      # bullet\n",
        "            '': '-',      # en dash\n",
        "            '': '--',     # em dash\n",
        "            '\"': '\"',      # left double quote\n",
        "            '\"': '\"',      # right double quote\n",
        "            ''': \"'\",      # left single quote\n",
        "            ''': \"'\",      # right single quote\n",
        "            '': '...',    # ellipsis\n",
        "            '': '[x]',    # checked checkbox\n",
        "            '': '[ ]',    # empty checkbox\n",
        "            '': '->',     # right arrow\n",
        "            '': '=>',     # double right arrow\n",
        "            '': 'v',      # check mark\n",
        "            '': 'x',      # cross mark\n",
        "            '': '~',      # approximately equal\n",
        "            '': '!=',     # not equal\n",
        "            '': '<=',     # less than or equal\n",
        "            '': '>=',     # greater than or equal\n",
        "        }\n",
        "\n",
        "        for char, replacement in replacements.items():\n",
        "            text = text.replace(char, replacement)\n",
        "\n",
        "        # Handle any remaining non-ASCII characters\n",
        "        return ''.join(c if ord(c) < 128 else '_' for c in text)\n",
        "\n",
        "    def markdown_to_pdf_reportlab(self, markdown_text):\n",
        "        \"\"\"Convert markdown to PDF using ReportLab (supports all Unicode)\"\"\"\n",
        "        buffer = io.BytesIO()\n",
        "        doc = SimpleDocTemplate(buffer, pagesize=letter, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)\n",
        "        story = []\n",
        "\n",
        "        lines = markdown_text.split('\\n')\n",
        "        list_items = []\n",
        "        in_list = False\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if in_list:\n",
        "                    # End the list\n",
        "                    story.append(ListFlowable(list_items, bulletType='bullet', leftIndent=20, bulletFontSize=12))\n",
        "                    list_items = []\n",
        "                    in_list = False\n",
        "                story.append(Spacer(1, 6))\n",
        "                continue\n",
        "\n",
        "            # Process headings\n",
        "            if line.startswith('# '):\n",
        "                if in_list:\n",
        "                    # End the list first\n",
        "                    story.append(ListFlowable(list_items, bulletType='bullet', leftIndent=20, bulletFontSize=12))\n",
        "                    list_items = []\n",
        "                    in_list = False\n",
        "                story.append(Paragraph(line[2:], self.heading1_style))\n",
        "\n",
        "            elif line.startswith('## '):\n",
        "                if in_list:\n",
        "                    # End the list first\n",
        "                    story.append(ListFlowable(list_items, bulletType='bullet', leftIndent=20, bulletFontSize=12))\n",
        "                    list_items = []\n",
        "                    in_list = False\n",
        "                story.append(Paragraph(line[3:], self.heading2_style))\n",
        "\n",
        "            # Process list items\n",
        "            elif line.startswith('- ') or line.startswith('* '):\n",
        "                content = line[2:]\n",
        "\n",
        "                # Handle checkboxes\n",
        "                if content.startswith('[ ] '):\n",
        "                    content = ' ' + content[4:]  # Empty checkbox\n",
        "                elif content.startswith('[x] ') or content.startswith('[X] '):\n",
        "                    content = ' ' + content[4:]  # Checked checkbox\n",
        "\n",
        "                list_items.append(ListItem(Paragraph(content, self.styles['Normal'])))\n",
        "                in_list = True\n",
        "\n",
        "            # Regular paragraph text\n",
        "            else:\n",
        "                if in_list:\n",
        "                    # End the list first\n",
        "                    story.append(ListFlowable(list_items, bulletType='bullet', leftIndent=20, bulletFontSize=12))\n",
        "                    list_items = []\n",
        "                    in_list = False\n",
        "\n",
        "                # Handle bold and italic text\n",
        "                if '**' in line or '*' in line:\n",
        "                    # Simple formatting: convert **text** to <b>text</b>\n",
        "                    line = re.sub(r'\\*\\*(.*?)\\*\\*', r'<b>\\1</b>', line)\n",
        "                    # Convert *text* to <i>text</i> but avoid replacing inside already processed tags\n",
        "                    line = re.sub(r'(?<!\\*)\\*(?!\\*)(.*?)(?<!\\*)\\*(?!\\*)', r'<i>\\1</i>', line)\n",
        "\n",
        "                story.append(Paragraph(line, self.styles['Normal']))\n",
        "\n",
        "        # Don't forget to add the last list if we're still in one\n",
        "        if in_list and list_items:\n",
        "            story.append(ListFlowable(list_items, bulletType='bullet', leftIndent=20, bulletFontSize=12))\n",
        "\n",
        "        # Build the PDF\n",
        "        doc.build(story)\n",
        "        pdf_data = buffer.getvalue()\n",
        "        buffer.close()\n",
        "        return pdf_data\n",
        "\n",
        "    def markdown_to_pdf(self, markdown_text):\n",
        "        \"\"\"Convert markdown to PDF using ReportLab with fallbacks\"\"\"\n",
        "        try:\n",
        "            # Try using reportlab first (best quality, supports Unicode)\n",
        "            return self.markdown_to_pdf_reportlab(markdown_text)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Error with ReportLab PDF generation: {str(e)}. Trying with sanitized text...\")\n",
        "            try:\n",
        "                # Try again with sanitized text\n",
        "                sanitized_text = self.sanitize_text(markdown_text)\n",
        "                return self.markdown_to_pdf_reportlab(sanitized_text)\n",
        "            except Exception as e2:\n",
        "                st.warning(f\"Error with sanitized ReportLab generation: {str(e2)}. Using basic FPDF...\")\n",
        "                # Last resort: use basic FPDF with ASCII only\n",
        "                return self._create_simple_pdf(markdown_text)\n",
        "\n",
        "    def _create_simple_pdf(self, markdown_text):\n",
        "        \"\"\"Fallback method with maximum compatibility\"\"\"\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "        # Sanitize the text to ensure it's ASCII-only\n",
        "        lines = self.sanitize_text(markdown_text).split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            # Skip empty lines\n",
        "            if not line.strip():\n",
        "                pdf.ln(5)\n",
        "                continue\n",
        "\n",
        "            # Process headers and text with simple formatting\n",
        "            if line.startswith('# '):\n",
        "                pdf.set_font(\"Arial\", 'B', 24)\n",
        "                pdf.multi_cell(0, 10, line[2:])\n",
        "                pdf.ln(5)\n",
        "            elif line.startswith('## '):\n",
        "                pdf.set_font(\"Arial\", 'B', 18)\n",
        "                pdf.multi_cell(0, 10, line[3:])\n",
        "                pdf.ln(3)\n",
        "            elif line.startswith('- ') or line.startswith('* '):\n",
        "                pdf.set_font(\"Arial\", '', 12)\n",
        "                content = line[2:]\n",
        "                pdf.multi_cell(0, 5, \"- \" + content)\n",
        "            else:\n",
        "                pdf.set_font(\"Arial\", '', 12)\n",
        "                pdf.multi_cell(0, 5, line)\n",
        "\n",
        "        try:\n",
        "            return pdf.output(dest='S').encode('latin1')\n",
        "        except Exception as e:\n",
        "            st.error(f\"Critical PDF generation error: {str(e)}\")\n",
        "            # Create an absolutely minimal PDF as last resort\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "            pdf.set_font(\"Arial\", 'B', 16)\n",
        "            pdf.cell(40, 10, 'Meeting Summary')\n",
        "            pdf.ln(15)\n",
        "            pdf.set_font(\"Arial\", '', 12)\n",
        "            pdf.cell(0, 10, 'Please download the Markdown version instead.')\n",
        "            return pdf.output(dest='S').encode('latin1')\n",
        "\n",
        "    def create_download_link(self, pdf_bytes, filename):\n",
        "        \"\"\"Create a download link for the PDF\"\"\"\n",
        "        b64 = base64.b64encode(pdf_bytes).decode()\n",
        "        href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{filename}\">Download PDF</a>'\n",
        "        return href\n",
        "\n",
        "\n",
        "def create_streamlit_app():\n",
        "    st.set_page_config(page_title=\"MeetingMind\", page_icon=\"\", layout=\"wide\")\n",
        "\n",
        "    st.title(\"MeetingMind: Meeting Audio to Summary Converter\")\n",
        "    st.write(\"Upload an audio file to generate a detailed meeting summary\")\n",
        "\n",
        "    # Initialize session state for storing processors\n",
        "    if 'processors_initialized' not in st.session_state:\n",
        "        st.session_state.processors_initialized = False\n",
        "        st.session_state.audio_processor = None\n",
        "        st.session_state.text_preprocessor = None\n",
        "        st.session_state.text_summarizer = None\n",
        "        st.session_state.pdf_generator = None\n",
        "\n",
        "    # Initialize processors if not already done\n",
        "    if not st.session_state.processors_initialized:\n",
        "        with st.spinner(\"Initializing models... This may take a few minutes...\"):\n",
        "            st.session_state.audio_processor = AudioProcessor()\n",
        "            st.session_state.text_preprocessor = TextPreprocessor()\n",
        "            st.session_state.text_summarizer = TextSummarizer()\n",
        "            st.session_state.pdf_generator = PDFGenerator()\n",
        "            st.session_state.processors_initialized = True\n",
        "\n",
        "    # Add explanation about name standardization feature\n",
        "    with st.expander(\" About Name Standardization\"):\n",
        "        st.write(\"\"\"\n",
        "        This application now includes automatic name standardization to handle misspelled or\n",
        "        mispronounced names in meeting transcripts. The system uses phonetic algorithms (Double Metaphone)\n",
        "        to identify when different spellings likely refer to the same person, ensuring consistent\n",
        "        references throughout your summary.\n",
        "\n",
        "        **How it works:**\n",
        "        1. The system detects potential names in the transcript\n",
        "        2. Names that sound similar are grouped together\n",
        "        3. The most likely correct spelling is chosen as the standard\n",
        "        4. All variants are replaced with the standard spelling\n",
        "\n",
        "        You'll see a section in your final summary that lists all standardized names.\n",
        "        \"\"\")\n",
        "\n",
        "    # File uploader\n",
        "    audio_file = st.file_uploader(\"Choose an audio file\", type=['mp3', 'wav', 'm4a'])\n",
        "\n",
        "    if audio_file is not None:\n",
        "        # Add options for name standardization\n",
        "        name_standardization = st.checkbox(\"Enable name standardization\", value=True,\n",
        "                                          help=\"Automatically detect and standardize names that sound similar but are spelled differently\")\n",
        "\n",
        "        if st.button(\"Generate Summary\"):\n",
        "            try:\n",
        "                # Create placeholder for progress bar and status\n",
        "                progress_placeholder = st.empty()\n",
        "                status_text = st.empty()\n",
        "\n",
        "                # Save uploaded file temporarily\n",
        "                temp_path = f\"temp_audio{Path(audio_file.name).suffix}\"\n",
        "                with open(temp_path, \"wb\") as f:\n",
        "                    f.write(audio_file.getbuffer())\n",
        "\n",
        "                # Process the audio file\n",
        "                with progress_placeholder.container():\n",
        "                    # Get audio duration first\n",
        "                    status_text.text(\" Analyzing audio duration...\")\n",
        "                    audio_duration = st.session_state.audio_processor.get_audio_duration(temp_path)\n",
        "                    formatted_duration = st.session_state.audio_processor.format_duration(audio_duration)\n",
        "\n",
        "                    # Step 1: Transcribe audio\n",
        "                    status_text.text(\" Transcribing audio... This may take a while...\")\n",
        "                    progress_bar = st.progress(0)\n",
        "                    transcript = st.session_state.audio_processor.transcribe(temp_path)\n",
        "                    progress_bar.progress(25)\n",
        "\n",
        "                    # Save original transcript for comparison if needed\n",
        "                    original_transcript = transcript\n",
        "\n",
        "                    # Step 2: Extract potential names (if enabled)\n",
        "                    if name_standardization:\n",
        "                        status_text.text(\" Detecting and standardizing names...\")\n",
        "                        # Pre-extract names to build the standardization dictionary\n",
        "                        st.session_state.text_preprocessor.name_standardizer.extract_names_from_text(transcript)\n",
        "\n",
        "                    # Step 3: Preprocess transcript\n",
        "                    status_text.text(\" Preprocessing transcript...\")\n",
        "                    processed_text = st.session_state.text_preprocessor.preprocess(transcript)\n",
        "                    progress_bar.progress(40)\n",
        "\n",
        "                    # Get name standardization report if enabled\n",
        "                    name_report = []\n",
        "                    if name_standardization:\n",
        "                        name_report = st.session_state.text_preprocessor.get_name_standardization_report()\n",
        "\n",
        "                    # Step 4: Split into chunks\n",
        "                    status_text.text(\" Splitting into chunks...\")\n",
        "                    chunks = st.session_state.text_preprocessor.split_into_chunks(processed_text)\n",
        "                    progress_bar.progress(50)\n",
        "\n",
        "                    # Step 5: Remove redundancies\n",
        "                    status_text.text(\" Removing redundancies...\")\n",
        "                    unique_chunks = st.session_state.text_summarizer.remove_redundancies(chunks)\n",
        "                    progress_bar.progress(60)\n",
        "\n",
        "                    # Step 6: Process chunks and generate summaries\n",
        "                    status_text.text(\" Generating summaries...\")\n",
        "                    chunk_progress = st.progress(0)\n",
        "                    grouped_chunks = st.session_state.text_summarizer.process_chunks(unique_chunks, chunk_progress)\n",
        "                    progress_bar.progress(80)\n",
        "\n",
        "                    # Step 7: Format summary\n",
        "                    status_text.text(\" Formatting summary...\")\n",
        "                    markdown_content = st.session_state.text_summarizer.format_markdown(\n",
        "                        grouped_chunks,\n",
        "                        formatted_duration,\n",
        "                        name_report if name_standardization else None\n",
        "                    )\n",
        "                    progress_bar.progress(90)\n",
        "\n",
        "                    # Step 8: Refine summary\n",
        "                    status_text.text(\" Refining summary.\")\n",
        "                    try:\n",
        "                        client = openai.OpenAI(\n",
        "                            api_key=GITHUB_TOKEN,\n",
        "                            base_url=BASE_URL\n",
        "                        )\n",
        "\n",
        "                        response = client.chat.completions.create(\n",
        "                            messages=[\n",
        "                                {\n",
        "                                    \"role\": \"system\",\n",
        "                                    \"content\": \"\"\"You are an expert meeting summary editor. Transform raw meeting summaries into well-structured, clear documents. Focus on:\n",
        "                        1. Enhancing readability and logical flow\n",
        "                        2. Eliminating redundancies - remove repeated information and consolidate similar points\n",
        "                        3. Organizing content into clearly defined sections\n",
        "                        4. Extracting and highlighting:\n",
        "                           - Meeting agenda points\n",
        "                           - Action items with owners and deadlines\n",
        "                           - Required follow-ups\n",
        "                           - Suggested topics for future meetings\n",
        "                        5. Preserving meeting duration and all unique, non-redundant information\n",
        "                        You may reduce content length, but only by removing duplicated information. Do not remove unique details.\"\"\"\n",
        "                                },\n",
        "                                {\n",
        "                                    \"role\": \"user\",\n",
        "                                    \"content\": f\"Refine this meeting summary to improve clarity and organization. Remove any repeated information while keeping all unique content.\\n\\nSUMMARY:\\n{markdown_content}\"\n",
        "                                }\n",
        "                            ],\n",
        "                            model=MODEL,\n",
        "                            temperature=0.5,\n",
        "                            max_tokens=10000\n",
        "                        )\n",
        "                        refined_summary = response.choices[0].message.content.strip()\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"Could not refine summary with GPT-4: {str(e)}. Using original summary.\")\n",
        "                        refined_summary = markdown_content\n",
        "\n",
        "                    progress_bar.progress(100)\n",
        "                    status_text.text(\" Summary generated successfully!\")\n",
        "\n",
        "                    # Generate PDF from the refined summary\n",
        "                    try:\n",
        "                        pdf_bytes = st.session_state.pdf_generator.markdown_to_pdf(refined_summary)\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"PDF generation encountered an error: {str(e)}. Trying fallback method...\")\n",
        "                        try:\n",
        "                            # Use the fallback method directly\n",
        "                            pdf_bytes = st.session_state.pdf_generator._create_simple_pdf(refined_summary)\n",
        "                        except Exception as e2:\n",
        "                            st.error(f\"Failed to generate PDF with fallback method: {str(e2)}\")\n",
        "                            pdf_bytes = None\n",
        "\n",
        "                    # Display results in tabs\n",
        "                    tab1, tab2, tab3 = st.tabs([\" Summary\", \" Details\", \" Name Standardization\"])\n",
        "\n",
        "                    with tab1:\n",
        "                        st.markdown(refined_summary)\n",
        "\n",
        "                        # Add download buttons\n",
        "                        col1, col2 = st.columns(2)\n",
        "\n",
        "                        with col1:\n",
        "                            st.download_button(\n",
        "                                label=\" Download Markdown\",\n",
        "                                data=refined_summary,\n",
        "                                file_name=\"meeting_summary.md\",\n",
        "                                mime=\"text/markdown\"\n",
        "                            )\n",
        "\n",
        "                        with col2:\n",
        "                            if pdf_bytes:\n",
        "                                st.download_button(\n",
        "                                    label=\" Download PDF\",\n",
        "                                    data=pdf_bytes,\n",
        "                                    file_name=\"meeting_summary.pdf\",\n",
        "                                    mime=\"application/pdf\"\n",
        "                                )\n",
        "                            else:\n",
        "                                st.error(\"PDF generation failed. Please download the Markdown version instead.\")\n",
        "\n",
        "                    with tab2:\n",
        "                        st.subheader(\"Processing Statistics\")\n",
        "                        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "                        with col1:\n",
        "                            st.metric(\"Meeting Duration\", formatted_duration)\n",
        "                        with col2:\n",
        "                            st.metric(\"Original Chunks\", len(chunks))\n",
        "                        with col3:\n",
        "                            st.metric(\"After Deduplication\", len(unique_chunks))\n",
        "                        with col4:\n",
        "                            st.metric(\"Topics Identified\", len(grouped_chunks))\n",
        "\n",
        "                        st.subheader(\"Identified Topics\")\n",
        "                        for topic in grouped_chunks.keys():\n",
        "                            st.write(f\"- {topic}\")\n",
        "\n",
        "                    with tab3:\n",
        "                        st.subheader(\"Name Standardization Results\")\n",
        "\n",
        "                        if name_standardization:\n",
        "                            name_groups = st.session_state.text_preprocessor.name_standardizer.get_name_groups()\n",
        "                            standardized_names = st.session_state.text_preprocessor.name_standardizer.get_name_variants()\n",
        "\n",
        "                            # Show statistics\n",
        "                            total_names = len(standardized_names)\n",
        "                            unique_names = len(name_groups)\n",
        "\n",
        "                            col1, col2 = st.columns(2)\n",
        "                            with col1:\n",
        "                                st.metric(\"Total Name Mentions\", total_names)\n",
        "                            with col2:\n",
        "                                st.metric(\"Unique Names (After Standardization)\", unique_names)\n",
        "\n",
        "                            # Show groups with multiple variants\n",
        "                            st.subheader(\"Name Standardization Groups\")\n",
        "\n",
        "                            has_groups = False\n",
        "                            for metaphone, names in name_groups.items():\n",
        "                                if len(names) > 1:  # Only show groups with variants\n",
        "                                    has_groups = True\n",
        "                                    standard_name = st.session_state.text_preprocessor.name_standardizer.metaphone_mapping.get(metaphone, list(names)[0])\n",
        "                                    variants = [name for name in names if name != standard_name]\n",
        "                                    if variants:  # Only report if there are actual variants\n",
        "                                        st.success(f\"**{standard_name}**  {', '.join(variants)}\")\n",
        "\n",
        "                            if not has_groups:\n",
        "                                st.info(\"No name variants requiring standardization were detected.\")\n",
        "\n",
        "                            # Option to see example snippets\n",
        "                            if st.checkbox(\"Show name occurrence examples from transcript\"):\n",
        "                                st.subheader(\"Name Occurrences in Transcript\")\n",
        "\n",
        "                                # Only display prominent names (appearing multiple times)\n",
        "                                prominent_names = [name for metaphone, names in name_groups.items()\n",
        "                                                for name in names\n",
        "                                                if len(re.findall(r'\\b' + re.escape(name) + r'\\b', original_transcript)) > 1]\n",
        "\n",
        "                                for name in sorted(set(prominent_names)):\n",
        "                                    # Find occurrences with surrounding context\n",
        "                                    matches = re.finditer(r'\\b' + re.escape(name) + r'\\b', original_transcript)\n",
        "                                    examples = []\n",
        "                                    for match in matches:\n",
        "                                        start = max(0, match.start() - 50)\n",
        "                                        end = min(len(original_transcript), match.end() + 50)\n",
        "                                        context = original_transcript[start:end]\n",
        "\n",
        "                                        # Highlight the name\n",
        "                                        highlighted = context.replace(name, f\"**{name}**\")\n",
        "                                        examples.append(highlighted)\n",
        "\n",
        "                                        if len(examples) >= 2:  # Limit to 2 examples per name\n",
        "                                            break\n",
        "\n",
        "                                    if examples:\n",
        "                                        with st.expander(f\"Examples of '{name}'\"):\n",
        "                                            for i, example in enumerate(examples):\n",
        "                                                st.markdown(f\"Example {i+1}: \\\"...{example}...\\\"\")\n",
        "                        else:\n",
        "                            st.info(\"Name standardization was not enabled for this summary. Enable it when generating a summary to see results here.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\" An error occurred: {str(e)}\")\n",
        "                st.error(\"Please try again with a different audio file or check the file format.\")\n",
        "\n",
        "            finally:\n",
        "                # Cleanup\n",
        "                if os.path.exists(temp_path):\n",
        "                    os.remove(temp_path)\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    # Add sidebar with information\n",
        "    with st.sidebar:\n",
        "        st.subheader(\" About\")\n",
        "        st.write(\"\"\"\n",
        "        This app converts meeting audio to detailed summaries with intelligent name standardization\n",
        "        to handle misspelled or mispronounced names.\n",
        "        \"\"\")\n",
        "\n",
        "        st.subheader(\" Supported Formats\")\n",
        "        st.write(\"- MP3\\n- WAV\\n- M4A\")\n",
        "\n",
        "        st.subheader(\" Name Standardization\")\n",
        "        st.write(\"\"\"\n",
        "        The app uses the Double Metaphone algorithm to identify phonetically similar names that likely refer\n",
        "        to the same person, even when spelling varies due to speech recognition errors or mispronunciation.\n",
        "\n",
        "        This helps ensure that the same person is consistently referenced throughout the summary.\n",
        "        \"\"\")\n",
        "\n",
        "        st.subheader(\" System Status\")\n",
        "        st.write(f\"Using device: {device}\")\n",
        "        if torch.cuda.is_available():\n",
        "            st.write(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Main entry point\n",
        "if __name__ == \"__main__\":\n",
        "    create_streamlit_app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67A1saDiMIPz",
        "outputId": "59f25bd9-2d4b-4771-9d60-fa6f23e0e8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: streamlit in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.43.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.2.3)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (10.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.25.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (19.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jay kanavia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ngrok ...\n",
            "Downloading ngrok: 0%\n",
            "Downloading ngrok: 1%\n",
            "Downloading ngrok: 2%\n",
            "Downloading ngrok: 3%\n",
            "Downloading ngrok: 4%\n",
            "Downloading ngrok: 5%\n",
            "Downloading ngrok: 6%\n",
            "Downloading ngrok: 7%\n",
            "Downloading ngrok: 8%\n",
            "Downloading ngrok: 9%\n",
            "Downloading ngrok: 10%\n",
            "Downloading ngrok: 11%\n",
            "Downloading ngrok: 12%\n",
            "Downloading ngrok: 13%\n",
            "Downloading ngrok: 14%\n",
            "Downloading ngrok: 15%\n",
            "Downloading ngrok: 16%\n",
            "Downloading ngrok: 17%\n",
            "Downloading ngrok: 18%\n",
            "Downloading ngrok: 19%\n",
            "Downloading ngrok: 20%\n",
            "Downloading ngrok: 21%\n",
            "Downloading ngrok: 22%\n",
            "Downloading ngrok: 23%\n",
            "Downloading ngrok: 24%\n",
            "Downloading ngrok: 25%\n",
            "Downloading ngrok: 26%\n",
            "Downloading ngrok: 27%\n",
            "Downloading ngrok: 28%\n",
            "Downloading ngrok: 29%\n",
            "Downloading ngrok: 30%\n",
            "Downloading ngrok: 31%\n",
            "Downloading ngrok: 32%\n",
            "Downloading ngrok: 33%\n",
            "Downloading ngrok: 34%\n",
            "Downloading ngrok: 35%\n",
            "Downloading ngrok: 36%\n",
            "Downloading ngrok: 37%\n",
            "Downloading ngrok: 38%\n",
            "Downloading ngrok: 39%\n",
            "Downloading ngrok: 40%\n",
            "Downloading ngrok: 41%\n",
            "Downloading ngrok: 42%\n",
            "Downloading ngrok: 43%\n",
            "Downloading ngrok: 44%\n",
            "Downloading ngrok: 45%\n",
            "Downloading ngrok: 46%\n",
            "Downloading ngrok: 47%\n",
            "Downloading ngrok: 48%\n",
            "Downloading ngrok: 49%\n",
            "Downloading ngrok: 50%\n",
            "Downloading ngrok: 51%\n",
            "Downloading ngrok: 52%\n",
            "Downloading ngrok: 53%\n",
            "Downloading ngrok: 54%\n",
            "Downloading ngrok: 55%\n",
            "Downloading ngrok: 56%\n",
            "Downloading ngrok: 57%\n",
            "Downloading ngrok: 58%\n",
            "Downloading ngrok: 59%\n",
            "Downloading ngrok: 60%\n",
            "Downloading ngrok: 61%\n",
            "Downloading ngrok: 62%\n",
            "Downloading ngrok: 63%\n",
            "Downloading ngrok: 64%\n",
            "Downloading ngrok: 65%\n",
            "Downloading ngrok: 66%\n",
            "Downloading ngrok: 67%\n",
            "Downloading ngrok: 68%\n",
            "Downloading ngrok: 69%\n",
            "Downloading ngrok: 70%\n",
            "Downloading ngrok: 71%\n",
            "Downloading ngrok: 72%\n",
            "Downloading ngrok: 73%\n",
            "Downloading ngrok: 74%\n",
            "Downloading ngrok: 75%\n",
            "Downloading ngrok: 76%\n",
            "Downloading ngrok: 77%\n",
            "Downloading ngrok: 78%\n",
            "Downloading ngrok: 79%\n",
            "Downloading ngrok: 80%\n",
            "Downloading ngrok: 81%\n",
            "Downloading ngrok: 82%\n",
            "Downloading ngrok: 83%\n",
            "Downloading ngrok: 84%\n",
            "Downloading ngrok: 85%\n",
            "Downloading ngrok: 86%\n",
            "Downloading ngrok: 87%\n",
            "Downloading ngrok: 88%\n",
            "Downloading ngrok: 89%\n",
            "Downloading ngrok: 90%\n",
            "Downloading ngrok: 91%\n",
            "Downloading ngrok: 92%\n",
            "Downloading ngrok: 93%\n",
            "Downloading ngrok: 94%\n",
            "Downloading ngrok: 95%\n",
            "Downloading ngrok: 96%\n",
            "Downloading ngrok: 97%\n",
            "Downloading ngrok: 98%\n",
            "Downloading ngrok: 99%\n",
            "Downloading ngrok: 100%\n",
            "                                                                                                    \n",
            "Installing ngrok ... \n",
            "                                                                                                    \n",
            "Authtoken saved to configuration file: C:\\Users\\Jay Kanavia\\AppData\\Local/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok streamlit\n",
        "!ngrok authtoken $2uUCzOWh8Tg8vWsILCbM4OENVt0_6eEm2qUtE9PG9sT3ktR3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOhxvabiMT2V",
        "outputId": "752fd577-1ea8-448e-e6bd-ebd91a4c5c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "103.84.198.188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "100    14  100    14    0     0      3      0  0:00:04  0:00:04 --:--:--     3\n"
          ]
        }
      ],
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln71PX_fMVqz",
        "outputId": "c8156522-4476-40aa-8c42-02e782c2daef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "added 22 packages in 4s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Install localtunnel globally\n",
        "!npm install -g localtunnel\n",
        "\n",
        "# Then run your streamlit app with tunnel\n",
        "!streamlit run app.py & lt --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNW4StjQel4Y"
      },
      "source": [
        "## If the above method fails to deploy tunnel then run this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S-LfN6CQ3-d",
        "outputId": "c1a28771-f509-4b0e-b5e7-ba73e60e47e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ngrok.zip\n",
            "  inflating: ngrok                   \n",
            "ngrok version 3.21.0\n",
            "Your Streamlit app is live at: NgrokTunnel: \"https://f818-34-125-81-73.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# 1. Download latest ngrok binary\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip\n",
        "!mv ngrok /usr/local/bin/ngrok\n",
        "!chmod +x /usr/local/bin/ngrok\n",
        "!ngrok version\n",
        "\n",
        "# 2. Python setup\n",
        "from pyngrok import conf, ngrok\n",
        "import os\n",
        "from threading import Thread\n",
        "\n",
        "# 3. Set config with updated ngrok binary\n",
        "config = conf.PyngrokConfig(ngrok_path=\"/usr/local/bin/ngrok\")\n",
        "ngrok.set_auth_token(\"2uUCXVtpmuTosp4zygymZKz1DOS_5RpePBgeZCEEMGzrS76SC\", pyngrok_config=config)\n",
        "\n",
        "# 4. Start streamlit in background\n",
        "def run_streamlit():\n",
        "    os.system('streamlit run /content/app.py --server.port 8501')\n",
        "\n",
        "thread = Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# 5. Connect ngrok tunnel (use addr in ngrok v3!)\n",
        "public_url = ngrok.connect(addr=\"localhost:8501\", proto=\"http\", pyngrok_config=config)\n",
        "print(\"Your Streamlit app is live at:\", public_url)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
